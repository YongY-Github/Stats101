[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability & Statistics 101",
    "section": "",
    "text": "Preface\nThis is an Introduction to Probability and Statistics book written with Quarto for the GA!\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nNote: This is a work-in-progress and expect revisions and updates intermittently!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is statistics?\nToday’s world is full of data. Data have become indispensable for making informed decisions, finding solutions to problems, understanding complex situations, and developing strategies that ultimately improve the lives of individuals and communities.1\nAlthough the terms data and information are often used interchangeably, they have distinct meanings. Data can be regarded as facts—especially numerical facts—that are collected for reference or analysis. Information, on the other hand, is knowledge communicated about some particular fact or phenomenon of interest.\nPut simply, statistics is a tool for extracting information from data. What the consumer of statistics ultimately seeks is information—something that helps them see through and make sense of the maze and dazzle of raw data.\nStatistics helps us create new understanding from data. Broadly speaking, there are two main branches of statistics:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Ch1.html#what-is-statistics",
    "href": "Ch1.html#what-is-statistics",
    "title": "1  Introduction",
    "section": "",
    "text": "NoteBig picture\n\n\n\nData by themselves are often messy and overwhelming. Statistics provides the tools that turn raw data into meaningful information.\n\n\n\n\n\nDescriptive statistics, which focus on organizing, summarizing, and presenting data in a clear and informative way.\nInferential statistics, which use data from a sample to make conclusions or draw inferences about some unknown aspect of a population.\n\n\n\n\n\n\n\nImportantKey idea\n\n\n\nDescriptive statistics describe what the data look like.\nInferential statistics help us learn about what we cannot directly observe.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Ch1.html#statistical-inference",
    "href": "Ch1.html#statistical-inference",
    "title": "1  Introduction",
    "section": "1.2 Statistical inference",
    "text": "1.2 Statistical inference\nDescriptive statistics rely on graphical summaries and numerical techniques. Statistical inference, in contrast, involves making estimates, predictions, and decisions about a population based on information obtained from a sample.\n\n\n\n\n\n\nFigure 1.1: Statistical Inference\n\n\n\nIn practice, what a statistician is often interested in is a population—a collection of all possible individuals, objects, or measurements of interest. Of particular importance is some numerical characteristic of the population, known as a parameter.\nBecause it is usually impossible (or impractical) to observe the entire population, a sample is drawn. From this sample, a statistic—a numerical quantity describing some feature of the sample—is calculated. Statistical inference then uses this statistic to say something about the corresponding population parameter.\nBefore we treat inferential statistics later in this book in detail, we begin with a discussion of basic statistical concepts that are best introduced through descriptive statistics, together with an introduction to elementary probability, which is essential for understanding statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Ch1.html#pioneers-of-statistics",
    "href": "Ch1.html#pioneers-of-statistics",
    "title": "1  Introduction",
    "section": "1.3 Pioneers of statistics",
    "text": "1.3 Pioneers of statistics\nThere are several approaches to doing statistics. In this book, we primarily adopt the classical approach, developed largely by Sir Ronald Aylmer Fisher (1890–1962)—a British statistician, geneticist, and professor. For his contributions, Fisher has been described as “a genius who almost single-handedly created the foundations for modern statistical science.” (Efron 1998)\nAnother important framework is the Bayesian approach, associated with Thomas Bayes (1702–1761), an English statistician, philosopher, and Presbyterian minister. Bayes is best known for formulating what is now called Bayes’ theorem, which will be introduced in a later chapter.2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Ch1.html#key-terms",
    "href": "Ch1.html#key-terms",
    "title": "1  Introduction",
    "section": "1.4 Key terms",
    "text": "1.4 Key terms\n\n1.4.1 Data\nData are characteristics or pieces of information—usually numerical—collected through observation. More formally, data are sets of values of qualitative or quantitative variables measured on one or more individuals or objects. A datum (the singular of data) refers to a single value of a single variable.\n\n\n1.4.2 Statistics\nStatistics is the discipline concerned with the collection, organization, analysis, interpretation, and presentation of data.\n\n\n1.4.3 Descriptive statistics\nDescriptive statistics refers to the process of summarizing and describing data, typically using tables, graphs, and numerical measures.\n\n\n1.4.4 Inferential statistics\nInferential statistics (or statistical inference) is the process of using data from a sample to draw conclusions about an underlying population or probability distribution.\n\n\n\n\n\n\nImportantChapter summary\n\n\n\nStatistics provides tools for turning raw data into meaningful information.\nDescriptive statistics summarize and present data, while inferential statistics use data from samples to draw conclusions about populations. Central to statistical reasoning are the ideas of populations, samples, parameters, and statistics, as well as the role of probability in making inference possible.\nThis chapter sets the conceptual foundation for the rest of the book and motivates the need for the descriptive and inferential tools developed in later chapters.\n\n\n\n\n\n\nEfron, Bradley. 1998. “R. A. Fisher in the 21st Century.” Statistical Science 13 (2): 95–122. https://doi.org/10.1214/ss/1028905930.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Ch1.html#footnotes",
    "href": "Ch1.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Think about the last decision you made using numbers—was it about money, health, grades, or something else?↩︎\nInterestingly, Bayes never published what would become his most famous accomplishment; his notes were edited and published after his death by Richard Price.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Ch2.html",
    "href": "Ch2.html",
    "title": "2  Graphical Summaries",
    "section": "",
    "text": "2.1 Types of data\nA defining characteristic of today’s information age is the abundance of data. Data in their raw form can be difficult to comprehend. One useful way to summarize a mass of data is through graphical or pictorial representations.\nMany such graphs—pie charts, bar graphs, and so on—are easy to understand and commonly used. In this chapter, we focus on the most important graphical representation in statistics: the histogram.1{.sidenote}\nData are not all the same. A helpful way to think about them is in three levels:\nOnce you know which kind of data you have, it becomes much easier to choose sensible statistics and graphs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch2.html#types-of-data",
    "href": "Ch2.html#types-of-data",
    "title": "2  Graphical Summaries",
    "section": "",
    "text": "Cardinal data\nOrdinal data\nNominal data\n\n\n\n2.1.1 Cardinal data\nCardinal data (sometimes further divided into interval and ratio data) are represented by real numbers—such as heights, weights, and prices. These are quantitative (numerical) data, so arithmetic operations are meaningful.\nFor example, with heights or prices, it makes sense to say someone is twice as tall or that one item is three times as expensive.\n\n\n2.1.2 Ordinal data\nOrdinal data are values for which only the order (ranking) is meaningful. A common example is a course evaluation scale such as:\n\nPoor = 1\n\nFair = 2\n\nGood = 3\n\nVery Good = 4\n\nExcellent = 5\n\nHere, arithmetic operations usually do not make sense. It would sound strange, for instance, to say:\n2 × Fair = Very Good.\nStill, ordinal data are useful because the ranking is clear: Excellent (5) is better than Very Good (4), which is better than Good (3), and so on. Importantly, that ordering remains the same even if we changed the numbers used to label the categories.\n\n\n2.1.3 Nominal data\nNominal data are usually qualitative or categorical. The numbers attached to categories serve only as labels. For example, marital status might be coded as:\n\nSingle = 1\n\nMarried = 2\n\nDivorced = 3\n\nWidowed = 4\n\nBecause these numbers are arbitrary, arithmetic operations are meaningless. It is absurd, for example, to say:\nmarried ÷ 2 = single.\nWith categorical data, the main meaningful calculation is counting—how often each category occurs. We often summarize nominal data using a frequency distribution, or a relative frequency distribution showing proportions.\n\n\n\n\n\n\nImportantKey idea\n\n\n\nThe type of data determines which graphs and summaries make sense.\n\n\n\n\n2.1.4 Why this matters\nKeeping the data type in mind helps determine which statistical tools—graphical or otherwise—are appropriate. Among graphical techniques, we will emphasize the histogram, which plays a central role in statistics and probability.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch2.html#the-histogram",
    "href": "Ch2.html#the-histogram",
    "title": "2  Graphical Summaries",
    "section": "2.2 The histogram",
    "text": "2.2 The histogram\nTo understand how a histogram works, let us look at a concrete example.\nTable 4.1 shows the percentage of families in each income class interval, measured in thousands of baht per year, for Bangkok residents in a recent year.\n\n\n\nTable 2.1: Annual income for a sample of Bangkok residents\n\n\n\n\n\nIncome (Baht ’000)\nPercentage\n\n\n\n\n0–30\n1\n\n\n30–60\n3\n\n\n60–90\n6\n\n\n90–120\n10\n\n\n120–150\n11\n\n\n150–210\n24\n\n\n210–270\n20\n\n\n270–390\n18\n\n\n390–600\n5\n\n\nOver 600\n1\n\n\n\n\n\n\nTo construct a histogram from Table 4.1, the first step is to draw the horizontal axis, representing income.\nA common mistake at this stage is to give each class interval the same width, regardless of its actual size. This leads to a misleading picture.\n\nInstead, the horizontal axis should reflect the actual width of each income class, as shown below.2\n\nThe next step is to draw the blocks. Students often make the mistake of setting the height of each block equal to the percentage in the table. The figure below shows what happens if this is done.\n\nThis gives a misleading impression—for example, it appears that there are more families earning over 390,000 baht than under 120,000 baht, which is not correct.\n\n\n\n\n\n\nWarningCommon pitfall\n\n\n\nIn a histogram, heights alone do not represent frequencies when class widths differ.\n\n\nBecause the class intervals are unequal, the height of each block must be calculated as:\n\npercentage ÷ class width (relative length)\n\nFor example, the interval 150–210 spans two 30,000-baht units. Dividing 24% by 2 gives the correct height. The vertical axis is therefore a density scale (percent per 30,000). The correct histogram is:\n\n\n\n\n\n\nFigure 2.1: Correct Histogram\n\n\n\nIn a histogram: - areas represent percentages (or probabilities) - heights represent crowding per horizontal unit\nAn important property to remember is that the total area under the histogram must equal 100 percent.\n\n\n\n\n\n\nImportantKey idea\n\n\n\nHistograms represent data through areas, not just heights.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch2.html#shapes-of-histograms",
    "href": "Ch2.html#shapes-of-histograms",
    "title": "2  Graphical Summaries",
    "section": "2.3 Shapes of histograms",
    "text": "2.3 Shapes of histograms\nHistograms come in many shapes. A histogram is symmetric if the two sides mirror each other around a central vertical line. The most famous symmetric histogram is the Gaussian (normal) distribution.\n\n\n\n\n\n\n\n\nFigure 2.2: Gaussian Distribution\n\n\n\n\n\nA skewed histogram is one with a long tail extending either to the right or to the left:\n\n\n\n\n\nHistograms can also be unimodal, with a single peak, or bimodal, with two peaks, and so on.\n\n\n\n\n\n\n\n\n\n\n\nImportantKey idea\n\n\n\nThe shape of a histogram provides valuable information about the distribution of the data, beyond what summary statistics alone can reveal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch2.html#quantitative-and-qualitative-variables",
    "href": "Ch2.html#quantitative-and-qualitative-variables",
    "title": "2  Graphical Summaries",
    "section": "2.4 Quantitative and qualitative variables",
    "text": "2.4 Quantitative and qualitative variables\nA variable is a characteristic that can vary from one individual to another.\nFor example, if we ask “How old are you?”, the variable of interest is age.\nIn this case, age is a quantitative variable, since it is usually expressed as a number.\nIn other cases, the question might be “What color is your hair?”\nHere, the variable is a qualitative variable, because the responses describe categories rather than numerical magnitudes.\nA useful distinction among quantitative variables is between discrete and continuous variables.\n\nA discrete variable takes on values in fixed steps. An example is the number of children in a household.\nA continuous variable can take on any value along a real line, such as height, weight, or income.\n\nWorking with quantitative variables is usually straightforward. Qualitative variables, however, are often handled by assigning arbitrary numerical codes to the possible responses. In practice, this means that a researcher codes qualitative information into nominal numerical data.\nFor example, a course evaluation might be coded as:\n\nPoor = 1\n\nFair = 2\n\nGood = 3\n\nVery Good = 4\n\nExcellent = 5\n\n\n\n\n\n\n\nWarningCommon pitfall\n\n\n\nDo not treat coded qualitative variables as numerical measurements. Arithmetic operations on these codes usually have no meaningful interpretation.\n\n\nIt is important to remember that these numbers are labels rather than true measurements, and arithmetic operations on them may not be meaningful.\nIt is also useful to distinguish between different data structures, such as:\n\nCross-sectional data, which observe many individuals at a single point in time\n\nTime-series data, which track a single unit over time\n\nPanel (or longitudinal) data, which follow multiple individuals over time\n\nThe statistical tools available to the researcher often depend on which of these data structures is being used.\n\n\n\n\n\n\nNotePause and think\n\n\n\nWhich of the variables you encounter in daily life—income, grades, or job titles—are quantitative, and which are qualitative?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch2.html#other-graphical-representations-of-data",
    "href": "Ch2.html#other-graphical-representations-of-data",
    "title": "2  Graphical Summaries",
    "section": "2.5 Other graphical representations of data",
    "text": "2.5 Other graphical representations of data\nThere are many graphical methods available for representing data. Some, such as bar graphs and pie charts, are widely known. Others, such as the stem-and-leaf plot or the ogive, are more specialized.\nMost introductory statistics textbooks discuss a variety of graphical summaries, and interested readers are encouraged to consult these sources.3\n\n\n\n\n\n\nImportantBig picture\n\n\n\nMany graphical tools exist, but for understanding distributions and probabilities, the histogram plays a uniquely central role.\n\n\n\n\n\n\n\n\nImportantChapter summary\n\n\n\nGraphical summaries provide a powerful way to explore and communicate patterns in data.\nDifferent types of data—cardinal, ordinal, and nominal—call for different graphical representations. Among these, the histogram plays a central role in statistics because it summarizes distributions through areas and connects naturally to ideas of probability.\nUnderstanding how to construct and interpret histograms, especially when class intervals are unequal, is essential for sound statistical analysis and for the topics that follow in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch2.html#exercises",
    "href": "Ch2.html#exercises",
    "title": "2  Graphical Summaries",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\n2.6.1 Conceptual questions\n\nExplain why graphical summaries are often more informative than tables when dealing with large data sets.\nGive one example each of:\n\na cardinal variable,\nan ordinal variable, and\na nominal variable.\n\nBriefly explain why arithmetic operations make sense for one but not for the others.\nWhy can assigning numbers to qualitative categories be misleading if those numbers are treated as measurements?\nA histogram shows two distributions with the same total area. What does this tell you about the data being represented?\n\n\n\n\n\n\n\nNoteCheck your understanding\n\n\n\nIf two histograms look very different but have the same total area, what does this imply about the percentages or probabilities they represent?\n\n\n\n\n\n2.6.2 Interpreting histograms\n\nConsider a histogram constructed using unequal class intervals.\n\nWhy must the vertical axis be interpreted as a density rather than a raw frequency?\nWhat would go wrong if heights were drawn directly from percentages?\n\nA histogram is right-skewed.\n\nWhat does this tell you about the distribution of the data?\nGive a real-world example of a variable that is likely to be right-skewed.\n\nExplain the difference between a unimodal and a bimodal histogram.\nWhat might cause a bimodal shape in real data?\nThe histogram below shows the distribution of final scores in a certain class.\n\n\nwhich block represents the people who scored between 60 and 80?\nTen percent scored between 20 and 40. About what percentage scored between 40 and 60?\nAbout what percentage scored over 60?\n9.A histogram of pocket-money used by EBA students in a week is shown above. No student used over Baht1,000 per month. The block over the class interval 200-500 is missing. How tall must it be?\n\n\nThe figure below shows a block of family income in a certain town. About what percent of the families in the city had incomes between Baht 15,000 and 25,000?\n\n\n\n\n\n\n\n\nWarningCommon pitfall\n\n\n\nStudents often confuse the height of bars with the area of bars in a histogram.\n\n\n\n\n\n2.6.3 Data structure and graphs\n\nFor each of the following data structures, suggest an appropriate graphical summary and explain why: - cross-sectional data, - time-series data, - panel (longitudinal) data.\nWhy is the histogram particularly important for later topics in probability and statistical inference?\n\n\n\n\n2.6.4 Optional challenge\n\nFind a real dataset (from news, government statistics, or online sources) and:\n\nclassify the variables as cardinal, ordinal, or nominal,\npropose at least one appropriate graphical summary for each variable,\nexplain why a histogram would or would not be appropriate.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch2.html#footnotes",
    "href": "Ch2.html#footnotes",
    "title": "2  Graphical Summaries",
    "section": "",
    "text": "Why might graphs be more informative than tables when data sets are large?↩︎\nClass intervals need not be equal; wider intervals may be used where data are sparse.↩︎\nSome recommended texts include Peck, Olsen, and Devore (2005, Chapter 3), Utts and Heckard (2006, Chapter 2), and Keller (2005, Chapters 2 and 3). The exercises at the end of this book also ask for graphical summaries not fully explained here, but which are worth exploring with the help of other introductory texts or online resources.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch3.html",
    "href": "Ch3.html",
    "title": "3  Numerical Summaries",
    "section": "",
    "text": "3.1 Measure of central location\nDescriptive statistics consist not only of graphical summaries, but also of numerical methods.\nSince we introduced the histogram in the previous chapter—the most important graphical summary of data—we now turn to basic numerical descriptive statistics.\nThese include:\nTogether, these numerical summaries help us describe data in a concise and informative way.\nThe arithmetic mean (or simply, the average) is the most widely used measure of central location. It is calculated by adding up all observed values and dividing by the number of observations.\n\\[\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n\\tag{3.1}\\]\nThe arithmetic mean, shown formally as Equation 3.1, is unique. Moreover, it has an important and intuitive property: the sum of the deviations of each observation from the mean is zero.\nAn interesting way to visualize this property is through a histogram. If the histogram were placed on a balance, it would balance exactly when supported at the arithmetic mean.\nThe median, another measure of central location, is the midpoint of the values in a dataset once the observations are ordered from the smallest to the largest.\nThe mode is a less commonly used measure of central location and refers to the value that occurs most frequently in the data.\nAn important advantage of the median is that it is not affected by extremely large or small values. For this reason, it is often a more reliable measure of central location when such extreme values are present.\nThe mode, on the other hand, can be interpreted as the “fashion category” of the data. In a histogram, it corresponds to the tallest bar or the highest peak.\nFor symmetrical histograms, such as the Gaussian (normal) distribution, the three measures coincide:\n\\[\n\\text{mean} = \\text{median} = \\text{mode}.\n\\]\nBut not all histograms are symmetric. If a histogram is skewed to the left or to the right, the three measures may differ.\nWhat is important to realize is that extreme values affect the average most, then the median, and least the mode. The arithmetic mean is therefore the most sensitive to extremely large or small values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch3.html#measure-of-central-location",
    "href": "Ch3.html#measure-of-central-location",
    "title": "3  Numerical Summaries",
    "section": "",
    "text": "ImportantKey idea\n\n\n\nThe mean, median, and mode summarize the center of the data, but they respond differently to extreme values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningCommon pitfall\n\n\n\nRelying only on the mean can be misleading when the data contain extreme values or are highly skewed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch3.html#measures-of-dispersion-or-spread",
    "href": "Ch3.html#measures-of-dispersion-or-spread",
    "title": "3  Numerical Summaries",
    "section": "3.2 Measures of dispersion or spread",
    "text": "3.2 Measures of dispersion or spread\n\nThe three histograms above all have the same average (or mean), yet they clearly look different. Describing a distribution using only its mean is therefore not sufficient. We also need to know how spread out the data are—that is, the dispersion or variability of the observations.\nThere are many measures of dispersion, including the range, the mean absolute deviation, the root mean square (r.m.s.), the standard deviation, the variance, the coefficient of variation, and the interquartile range. We now focus on the most important of these.\n\n3.2.1 The standard deviation\nThe standard deviation, abbreviated as SD, is the most important measure of dispersion used in this book. There are several ways to define and compute it, but the most intuitive approach is based on the idea of the root mean square (r.m.s.).\nThe starting point is the notion of a deviation, which is simply the distance of an observation from the average. Mathematically, the deviation of an observation (\\(X_i\\)) from the mean (\\(\\overline{X}\\)) is\n\\[\n\\text{deviation} = X_i - \\overline{X}.\n\\]\nThe term root mean square is a useful reminder of the steps involved—provided one reads it backwards:\n\nSquare all the entries\n\nTake the mean of the squares\n\nTake the square root of the mean\n\nIn equation form, the root mean square of a variable \\(X\\) can be written as\n\\[\n\\text{r.m.s. of } X = \\sqrt{X^2}.\n\\]\nThis, however, is just a mathematical operation.\nThe standard deviation is defined more specifically as the root mean square of the deviations from the mean, that is,\n\\[\n\\text{SD of } X = \\text{r.m.s. (deviation from the average)}.\n\\]\nIn simple terms, the standard deviation tells us how far, on average, the observations lie from their mean.\n\n\n3.2.2 A numerical example\nTo make this concrete, consider the list of numbers\n\\[\n20,\\; 10,\\; 15,\\; 15.\n\\]\nThe average \\(\\overline{X}\\) is 15. The deviations from the mean are therefore\n\\[\n5,\\; -5,\\; 0,\\; 0.\n\\]\nApplying the r.m.s. procedure gives the standard deviation:\n\\[\n\\begin{aligned}\n\\text{SD}\n&= \\sqrt{\\frac{5^2 + (-5)^2 + 0^2 + 0^2}{4}} \\\\\n&= \\sqrt{\\frac{25 + 25 + 0 + 0}{4}} \\\\\n&= \\sqrt{\\frac{50}{4}} = \\sqrt{12.5} \\approx 3.5.\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTipTry This: Exploring Mean and SD\n\n\n\nPart 1: Shifting and Scaling\nFind the mean and standard deviation (SD) for the following lists:\n(i) \\(1, 3, 4, 5, 7\\) (ii) \\(6, 8, 9, 10, 12\\) (iii) \\(3, 9, 12, 15, 21\\)\nQuestion: What do you notice about how the mean and SD change across these three lists?\n\nPart 2: Symmetry and Signs\nFind the mean and SD for these two lists:\n(i) \\(5, -4, 3, -1, 7\\) (ii) \\(-5, 4, -3, 1, -7\\)\n\n\n\nAdding a constant to a list does not change the standard deviation.\nMultiplying a list by some constant \\(k\\) multiplies the standard deviation by \\(k\\)\nChanging each sign of elements in a list does not change teh standard deviation\n\n\n\n3.2.3 Population and sample standard deviation\nIt is important to distinguish between the population standard deviation and the sample standard deviation, although this distinction becomes more relevant in statistical inference.\nWhen computing the population standard deviation, we divide by \\(n\\), the number of observations:\n\\[\n\\text{SD} = \\sqrt{\\frac{\\sum (X - \\overline{X})^2}{n}}\n\\tag{3.2}\\]\nWhen computing the standard deviation of a sample, we divide by \\((n - 1)\\):\n\\[\n\\text{SD}^+ = \\sqrt{\\frac{\\sum (X - \\overline{X})^2}{n - 1}}\n\\tag{3.3}\\]\nWe use \\(\\text{SD}^+\\) to denote the sample standard deviation. It is numerically larger than \\(\\text{SD}\\) because we divide by the smaller number \\((n - 1)\\) rather than \\(n\\).1\n\n\n3.2.4 Variance and the coefficient of variation\nAnother measure of dispersion is the variance, which is simply the square of the standard deviation. While the standard deviation is measured in the same units as the data, the variance is measured in squared units, making interpretation less direct.2\nComparing variability across variables measured in different units is often difficult. A relative measure of dispersion, known as the coefficient of variation (CV), is sometimes used. The CV is defined as the ratio of the standard deviation to the mean, expressed as a percentage.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch3.html#measures-of-relative-standing",
    "href": "Ch3.html#measures-of-relative-standing",
    "title": "3  Numerical Summaries",
    "section": "3.3 Measures of relative standing",
    "text": "3.3 Measures of relative standing\n\n\n\n\n\n\nImportantKey idea\n\n\n\nMeasures of relative standing tell us where an observation lies within a distribution, not just its magnitude.\n\n\n\n3.3.1 Percentiles and Quartiles\nThere are two important and closely related measures of relative standing:\n\nPercentiles\nQuartiles\n\nThe \\(p\\)th percentile is the value for which \\(p\\%\\) of the observations are less than (or equal to) that value and \\((100 - p)\\%\\) are greater than (or equal to) that value.\nFor example:\n\nThe 100th percentile is the maximum value (since 100% of observations are below or equal to it).\nThe 50th percentile is the median, the value that splits the data in half.\nThe 25th percentile and 75th percentile are called the first quartile (\\(Q_1\\)) and third quartile (\\(Q_3\\)), respectively.\n\n\n\n\n3.3.2 Locating a Percentile\nGiven an ordered dataset of size \\(n\\), the location of the \\(p\\)th percentile can be found using:\n\\[\nL_p = (1 + n)\\frac{p}{100}\n\\]\nwhere:\n\n\\(n\\) = number of observations\n\n\\(p\\) = desired percentile\n\nIf \\(L_p\\) is not an integer, interpolate between the surrounding observations.\n\n\n\n3.3.3 Example\nConsider the ordered dataset:\n\\[\n\\{0, 1, 5, 7, 8, 9, 12, 14, 22, 23\\}\n\\]\nThis represents the number of internet sites visited by 10 students.\nHere, \\(n = 10\\).\nFirst Quartile (\\(Q_1\\))\nTo find the 25th percentile:\n\\[\nL_{25} = (10 + 1)\\frac{25}{100} = 2.75\n\\]\nThis means the 25th percentile lies 0.75 of the way between the 2nd and 3rd observations.\n\n2nd observation = 1\n\n3rd observation = 5\n\nInterpolate:\n\\[\nQ_1 = 1 + 0.75(5 - 1) = 1 + 3 = 4\n\\]\nMedian (\\(Q_2\\))\nThe 50th percentile:\n\\[\nL_{50} = (10 + 1)\\frac{50}{100} = 5.5\n\\]\nThis lies halfway between the 5th and 6th observations:\n\n5th value = 8\n\n6th value = 9\n\nSo,\n\\[\nQ_2 = 8.5\n\\]\nThird Quartile (\\(Q_3\\))\nThe 75th percentile:\n\\[\nL_{75} = (10 + 1)\\frac{75}{100} = 8.25\n\\]\nThis lies 0.25 of the way between the 8th and 9th observations:\n\n8th value = 14\n\n9th value = 22\n\nSo,\n\\[\nQ_3 = 14 + 0.25(22 - 14) = 14 + 2 = 16\n\\]\n\n\n3.3.4 Interquartile Range (IQR)\nThe interquartile range measures the spread of the middle 50% of the data and is defined as\n\\[\nIQR = Q_3 - Q_1.\n\\]\nUsing our results:\n\\[\nIQR = 16 - 4 = 12.\n\\]\nThis means that the central half of the observations lie within a range of 12 units.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch3.html#exercises",
    "href": "Ch3.html#exercises",
    "title": "3  Numerical Summaries",
    "section": "4.1 Exercises",
    "text": "4.1 Exercises\n\n4.1.1 Conceptual questions\n\nExplain why the mean alone is often insufficient to describe a dataset.\nCompare the mean, median, and mode.\nIn what situations might one be preferred over the others?\nWhy is the median often a better measure of central location than the mean when data contain extreme values?\nExplain why two datasets can have the same mean but very different standard deviations.\n\n\n\n\n\n\n\nNoteCheck your understanding\n\n\n\nIf two datasets have the same mean and median, must they have the same standard deviation? Explain.\n\n\n\n\n\n4.1.2 Understanding dispersion\n\nDescribe in words what the standard deviation measures.\nWhy is it based on deviations from the mean rather than deviations from the median?\nExplain the role of the root mean square (r.m.s.) in defining the standard deviation.\nWhich has larger r.m.s? the list \\(7,7,7,7\\) or \\(7,-7,7,-7\\)?\nWhat is the r.m.s of \\(17,17,17,17,17\\)? What is the SD?\nCan the SD ever be negative?\nFor a list of positive numbers, can the SD ever be larger than the average?\nWhy does the variance have squared units, and why can this make interpretation difficult?\n\n\n\n\n\n\n\nWarningCommon pitfall\n\n\n\nA small standard deviation does not imply that the data are “small”—only that they are tightly clustered around the mean.\n\n\n\n\n\n4.1.3 Measures of relative standing\n\nExplain what it means for a value to be at the 75th percentile of a distribution.\nWhy is the median equal to the 50th percentile?\nDescribe how a box plot summarizes information about central location, dispersion, and relative standing.\n\n\n\n\n4.1.4 Numerical practice\n\nConsider the dataset: \\[\n5,\\; 8,\\; 2,\\; 9,\\; 5,\\; 3,\\; 7,\\; 4,\\; 2,\\; 7,\\; 4,\\; 10,\\; 4,\\; 3,\\; 5.\n\\]\n\n\n\nCompute the mean, median, and mode.\n\nCompute the standard deviation, SD and variance.\n\nExplain what each statistic tells you about the data.\n\nCalculate their 25th and 75th percentiles.\n\nFind the inter-quartile range (IQR).\n\nCalculate the coefficient of variation for the two lists.\n\nWhat is the mean absolute deviation for both lists?\n\nDraw a boxplot for the lists of numbers.\n\n\n\n\n4.1.5 Optional challenge\n\nFind a real dataset (for example, income, test scores, or prices):\n\n\ncompute at least two measures of central location,\ncompute at least one measure of dispersion,\nexplain which statistics are most informative and why.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch3.html#footnotes",
    "href": "Ch3.html#footnotes",
    "title": "3  Numerical Summaries",
    "section": "",
    "text": "This is for notional purposes only. Although it may seem counterintuitive, this adjustment corrects for the fact that samples tend to underestimate population variability.↩︎\nVariances are especially useful in algebraic manipulations. For example, variances can be added, whereas standard deviations cannot.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "Ch4.html",
    "href": "Ch4.html",
    "title": "4  Correlation",
    "section": "",
    "text": "4.1 Graphical representation\nSo far, we have focused on a single variable—what is known as univariate data—and examined their graphical and numerical summaries. We now turn to bivariate data, that is, data involving two variables.\nAs economists, we are often interested in understanding relationships between variables—for example, wages and education, CEO performance and salaries, economic growth and foreign aid, and many others. As with univariate data, both graphical and numerical summaries can be used.\nA contingency table lists the frequency of each combination of values of two categorical variables. For example, a survey of 2,237 people recording gender and handedness might produce the following table.1\nWhen dealing with cardinal (quantitative) data, the relationship between two variables is better visualized using a scatter diagram. The figure below shows the association between the duration of eruptions of Old Faithful and the waiting time between eruptions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Ch4.html#graphical-representation",
    "href": "Ch4.html#graphical-representation",
    "title": "4  Correlation",
    "section": "",
    "text": "NoteKey graphical tools for two variables\n\n\n\nThe two most important graphical summaries for bivariate data are:\n\nContingency tables, usually for categorical variables\n\nScatter diagrams, usually for quantitative variables\n\n\n\n\n\n\n\nTable 4.1: Gender and handedness\n\n\n\n\n\n\nMale\nFemale\n\n\n\n\nRight-handed\n934\n1070\n\n\nLeft-handed\n113\n92\n\n\nAmbidextrous\n20\n8\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Waiting time and eruption duration of Old Faithful",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Ch4.html#correlation",
    "href": "Ch4.html#correlation",
    "title": "4  Correlation",
    "section": "4.2 Correlation",
    "text": "4.2 Correlation\nGraphs are informative, but they also have limitations. For this reason, we need numerical summaries that describe the relationship between two variables.\nIn earlier chapters, we introduced the mean and standard deviation as the most important numerical summaries for univariate data. One might therefore ask: Why are these not sufficient for bivariate data?\n\n\n\n\n\n\nImportantWhy means and SDs are not enough\n\n\n\nTwo datasets can have the same mean and standard deviation for each variable, yet exhibit very different relationships between those variables.\n\n\nConsider a university that has offered two statistics sections, \\(A\\) and \\(B\\), over several years. For each student, midterm and final exam scores (out of 200) are recorded and plotted.\n\n\n\n\n\n\nFigure 4.2: Students’ score: Section A and B\n\n\n\nPoints lying on the 45\\(^\\circ\\) line represent students whose midterm and final scores are the same.\nIf there is a strong association, knowing one variable helps predict the other. If the association is weak, knowing one variable provides little predictive power.\nFor example, a student in section \\(A\\) who scored 150 on the midterm could plausibly score anywhere between 60 and 160 on the final. In section \\(B\\), a student with the same midterm score would likely score between 100 and 160. Midterm scores therefore provide better predictive information in section \\(B\\).\nMore strikingly, the means and standard deviations of both midterm and final scores are almost identical across the two sections. This means that we cannot distinguish the sections using only these summaries.\n\n\n\n\n\n\nImportantWe need a new measure\n\n\n\nTo capture how tightly points cluster around a line in a scatter diagram, we introduce the correlation coefficient, denoted by \\(r\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Ch4.html#correlation-coefficient",
    "href": "Ch4.html#correlation-coefficient",
    "title": "4  Correlation",
    "section": "4.3 Correlation coefficient",
    "text": "4.3 Correlation coefficient\nThe correlation coefficient is a pure number between \\(-1\\) and \\(1\\).\n\n\n\n\n\n\nImportant\n\n\n\n\\[\n-1 \\leq Corr(X,Y) \\leq 1\n\\]\n\n\nFurthermore:\n\n\\(r = 1\\) or \\(r = -1\\) indicates perfect correlation\nA positive value of \\(r\\) indicates a positive relationship\nA negative value of \\(r\\) indicates a negative relationship\n\nThe figures below show two variables with the same mean (3) and standard deviation (1), but different correlations.\n\n\n\n\n\n\nFigure 4.3: Positive correlation\n\n\n\n\n\n\n\n\n\nFigure 4.4: Negative correlation\n\n\n\n\n\n\n\n\n\nNoteA technical detail\n\n\n\nIf either variable has a standard deviation equal to zero, then \\(r = 0\\). Correlation requires variation in both variables.\n\n\n\n4.3.1 How correlation is computed\nTo compute the correlation coefficient:\n\nConvert each variable into standard units\nMultiply the standard units pairwise\nTake the average of these products\n\nStandard units measure distance from the mean in terms of standard deviations.\nFor the list \\(1, 3, 4, 5, 7\\), the mean is 4 and the standard deviation is 2. The values in standard units are: \\(-1.5,\\; -0.5,\\; 0,\\; 0.5,\\; 1.5\\)\n\n\n\n\n\n\nImportantStandard units\n\n\n\nStandard units measure how far a value lies from the mean relative to the spread of the data.\n\n\nNow consider the five points: \\((1,5), (3,9), (4,7), (5,1), (7,13)\\).\nThe mean of \\(X\\) is 4 with \\(SD_X = 2\\), and the mean of \\(Y\\) is 7 with \\(SD_Y = 4\\).\n\n\n\nTable 4.2: Calculating the correlation coefficient\n\n\n\n\n\nX\nY\n\\(SU_X\\)\n\\(SU_Y\\)\nProduct\n\n\n\n\n1\n5\n-1.5\n-0.5\n0.75\n\n\n3\n9\n-0.5\n0.5\n-0.25\n\n\n4\n7\n0.0\n0.0\n0.0\n\n\n5\n1\n0.5\n-1.5\n-0.75\n\n\n7\n13\n1.5\n1.5\n2.25\n\n\n\n\n\n\nThe correlation coefficient is the average of the last column, which equals \\(r = 0.4\\).\n\n\n4.3.2 Why this method matters\nThis approach shows that correlation:\n\nhas no units\nis unchanged by rescaling variables\nmeasures relative, not absolute, clustering\n\n\nThere are many other ways to compute the correlation coefficient, but we will stick to the above method as it reveals best how the correlation coefficient works.\nIn many textbooks, the formula for correlation is written as:2\n\n\n\n\n\n\nImportantCorrelation coefficient\n\n\n\n\\[\nCorr(X,Y) = \\frac{\\sum_{i=1}^{n} (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i - \\overline{X})^2} \\sqrt{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}}\n\\]\nAnother definition commonly used is:\n\\[\nCorr(X,Y) = \\displaystyle \\frac{\\text{Covariance}(X,Y)}{SD_x SD_y}\n\\]\n\n\n\n\n\n\n\n\nTipExercise: Correlation coefficients\n\n\n\nCompute the correlation coefficient for each of the following datasets.\n(a)\n\n\n\n\n\\(X\\)\n1\n1\n1\n1\n2\n2\n2\n3\n3\n4\n\n\n\n\n\\(Y\\)\n5\n3\n5\n7\n3\n3\n1\n1\n1\n1\n\n\n\n\n(b)\n\n\n\n\n\\(X\\)\n1\n1\n1\n1\n2\n2\n2\n3\n3\n4\n\n\n\n\n\\(Y\\)\n1\n2\n1\n3\n1\n4\n1\n2\n2\n3\n\n\n\n\n(c)\n\n\n\n\n\\(X\\)\n1\n1\n1\n1\n2\n2\n2\n3\n3\n4\n\n\n\n\n\\(Y\\)\n2\n2\n2\n2\n4\n4\n4\n6\n6\n8\n\n\n\n\nQuestion.\nWhat do you observe about the values of the correlation coefficients across the three cases?\n\n\nNote that the correlation coefficient is a pure number without any units, which is noted by the conversion to standard units.\nMoreover, the correlation coefficient is *not** affected by any of the three changes: - 1) interchanging the two variables, - 2) adding the same number to all the values of one variable, and - 3) multiplying all the values of one variable by the same positive number.\nThis basically means that changing the scale of any of the variables leaves the correlation coefficient unchanged.\n\n\n\n\n\nBe warned, looks can be deceiving. A quick glance at the above scatter plots would easily convince someone that the points in (a) is more tightly clustered, and thus having a correlation coefficient closer to 1, than (b). This cannot be further than the truth, because in fact = 0.7 for both diagrams. (b) is simply a magnification of (a). What is important to realize is that calculating involves converting variables to standard units (deviation from average divided by SD) and therefore it measures clustering not in absolute terms, but in relative terms (more specifically, relative to their standard deviations). That is, although plot (a) may appear more tightly clustered than plot (b), both have \\(r = 0.7\\). Plot (b) is simply a magnified version of plot (a). Correlation measures clustering relative to standard deviation, not in absolute terms.\n\n\n\n\n\n\nWarningCorrelation measures linear association\n\n\n\nThe correlation coefficient captures linear relationships only. Nonlinear relationships may have low correlation even when variables are strongly related.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Ch4.html#association-is-not-causation",
    "href": "Ch4.html#association-is-not-causation",
    "title": "4  Correlation",
    "section": "4.4 Association is not causation",
    "text": "4.4 Association is not causation\nThe last piece of warning when using correlation coefficients is to realize that what we have is a representation of association, that is how the variables move relative to each other. The correlation coefficient does not establish causation. This is best left to economic theory, which is called upon to interpret and explain the data.\n\n\n\n\n\n\nImportant\n\n\n\nCorrelation measures association, not causation.\n\n\nA positive or negative correlation does not explain why variables move together. Establishing causation requires economic theory and careful empirical reasoning.\nFor example, education is often thought to cause higher wages because more educated individuals tend to earn more. However, a third factor—such as individual determination or motivation—may influence both education and wages.\nSuch a factor is known as a confounding variable, and its presence can invalidate causal interpretations based solely on correlation.\n\n\n\n\n\n\nImportantTakeaway\n\n\n\nCorrelation is a powerful descriptive tool, but causal claims require much more than a high value of \\(r\\).\n\n\n\n\n\n\n\n\nImportantChapter summary\n\n\n\nCorrelation provides a numerical summary of the strength and direction of linear association between two variables.\nScatter diagrams help visualize relationships, while the correlation coefficient measures how tightly points cluster around a straight line. However, correlation alone cannot establish causation, and careful reasoning is required to interpret empirical relationships correctly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Ch4.html#exercises",
    "href": "Ch4.html#exercises",
    "title": "4  Correlation",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\n4.5.1 Conceptual questions\n\nExplain the difference between univariate and bivariate data.\nWhy are scatter diagrams useful for studying relationships between quantitative variables?\nExplain why two datasets can have identical means and standard deviations but very different correlations.\n\n\n\n\n4.5.2 Understanding correlation\n\nWhat does the sign of the correlation coefficient indicate?\nWhat does the magnitude of the correlation coefficient tell us?\nWhy must both variables exhibit variation for correlation to be meaningful?\n\n\n\n\n\n\n\nWarningCommon pitfall\n\n\n\nA high correlation does not imply a causal relationship.\n\n\n\n\n\n4.5.3 Computation and interpretation\n\nExplain how converting variables to standard units removes units from the correlation coefficient.\nWhy is correlation unaffected by changes in scale or units?\nGive an example of a nonlinear relationship that might have low correlation.\n\n\n\n\n4.5.4 Optional challenge\n\nFind two real-world variables that are correlated.\nDiscuss whether the relationship is likely causal, and identify possible confounding variables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Ch4.html#footnotes",
    "href": "Ch4.html#footnotes",
    "title": "4  Correlation",
    "section": "",
    "text": "For more on the relationship between categorical variables, a useful text is Utts and Heckard (2006, Chapter6).↩︎\nSometimes, we can write cov(X,Y) = (avg. of products XY) \\(-\\) (avg. of X) \\(\\times\\) (avg. of Y).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Ch6.html",
    "href": "Ch6.html",
    "title": "5  The Normal Distribution",
    "section": "",
    "text": "5.1 The normal probability density function\nThe Gaussian (normal) distribution is perhaps the most important continuous probability distribution in statistics.\nIts importance stems largely from the Central Limit Theorem, which states that the sum (or average) of a large number of independent random variables will be approximately normally distributed — regardless of their individual distributions.\nAny random variable that can be regarded as the sum of many small, independent contributions is therefore likely to follow an approximately normal distribution.\nWe return to this powerful result in Chapter 11.\nA continuous random variable is said to be normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\) if its probability density function is\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\ne^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.\n\\]\nThe normal distribution is completely determined by two parameters:\nWe write:\n\\[\nX \\sim N(\\mu, \\sigma^2).\n\\]\nAlthough the formula looks intimidating, in practice we rarely manipulate it directly. Instead, we standardize and use tables (or software).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#the-normal-probability-density-function",
    "href": "Ch6.html#the-normal-probability-density-function",
    "title": "5  The Normal Distribution",
    "section": "",
    "text": "the mean \\(\\mu\\)\nthe variance \\(\\sigma^2\\)\n\n\n\n\n\n\n\n\n\nImportantKey properties\n\n\n\n\nThe distribution is symmetric about its mean.\nIt extends from \\(-\\infty\\) to \\(+\\infty\\).\nThe total area under the curve equals 1.\nMean = median = mode.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#the-bell-shaped-curve",
    "href": "Ch6.html#the-bell-shaped-curve",
    "title": "5  The Normal Distribution",
    "section": "5.2 The bell-shaped curve",
    "text": "5.2 The bell-shaped curve\n\n\n\n\n\n\nFigure 5.1: Heights\n\n\n\nThe empirical rule (also called the 68–95–99.7 rule) states:\n\nAbout 68% of observations lie within \\(\\pm 1\\) standard deviation.\nAbout 95% lie within \\(\\pm 2\\) standard deviations.\nAbout 99.7% lie within \\(\\pm 3\\) standard deviations.\n\nThis rule provides quick approximations without using tables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#the-role-of-mean-and-variance",
    "href": "Ch6.html#the-role-of-mean-and-variance",
    "title": "5  The Normal Distribution",
    "section": "5.3 The role of mean and variance",
    "text": "5.3 The role of mean and variance\nThe normal distribution forms a family of curves.\n\nChanging \\(\\mu\\) shifts the curve left or right.\nChanging \\(\\sigma\\) stretches or compresses the curve.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#standardization-and-the-standard-normal-distribution",
    "href": "Ch6.html#standardization-and-the-standard-normal-distribution",
    "title": "5  The Normal Distribution",
    "section": "5.4 Standardization and the standard normal distribution",
    "text": "5.4 Standardization and the standard normal distribution\nTo make probability calculations manageable, we standardize values.\nThe standardized value (or z-score) is:\n\\[\nz = \\frac{X - \\mu}{\\sigma}.\n\\]\n\n\n\n\n\n\nNoteThe standard normal distribution\n\n\n\nAfter standardization,\n\\[\nz \\sim N(0,1).\n\\]\nIt has mean 0 and standard deviation 1.\n\n\nStandardization allows us to use a single universal table.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#using-the-standard-normal-table",
    "href": "Ch6.html#using-the-standard-normal-table",
    "title": "5  The Normal Distribution",
    "section": "5.5 Using the standard normal table",
    "text": "5.5 Using the standard normal table\nSuppose we want the area between the mean and \\(z = 1.20\\).\nFrom the table:\n\\[\nPr(0 \\le z \\le 1.20) = 0.3849.\n\\]\nInstead of solving\n\\[\nPr(0 \\le z \\le 1.20)\n= \\int_0^{1.20} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz,\n\\]\nwe simply look up the value.\n\n\n\n\n\n\nLocate the \\(z\\) value in the first column.\nRead the corresponding area from the table.\n\nHence, \\(Pr(0 \\le z \\le 1.20) = 38.49\\%\\).\n\n5.5.1 Another Example:\nFor \\(z = 1.35\\).\n\nFind 1.3 in the first column.\nMove across to 0.05.\nThe table value is 0.4115.\n\nHence, \\(Pr(0 \\le z \\le 1.38) = 41.15 \\%\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#areas-between-two-z-scores",
    "href": "Ch6.html#areas-between-two-z-scores",
    "title": "5  The Normal Distribution",
    "section": "5.6 Areas between two z-scores",
    "text": "5.6 Areas between two z-scores\nTo find:\n\\[\nPr(z_1 \\le z \\le z_2),\n\\]\n\nLook up both areas from the mean.\nAdd or subtract appropriately.\n\n\n5.6.1 Example\nTo find:\n\\[\nPr(-1 \\le z \\le 2),\n\\]\nFrom the standard normal table, \\(Pr(0 \\le z \\le 2) = 0.4772\\), and \\(Pr(0 \\le z \\le 1) = 0.3413 = Pr(-1 \\le z \\le 0)\\), due to symmetry.\nHence, \\(Pr(-1 \\le z \\le 2) = 0.4772 + 0.3413 = 0.8185\\).\n\nThe table is make life even easier.\nIt allows us easily to look up the probabilities between the mean and some \\(z\\)-score.\nBut, even more, it allows to look up a \\(z\\)-score given some probability. This is the same as trying to solve:\n\\[\nPr(0 \\leq z \\leq z^*) = 0.45 = \\int_0^{z^*} \\frac{1}{\\sigma \\sqrt{2\\pi}}\ne^{-\\frac{1}{2}z^2} dx\n\\]\nwhich is quite intimidating!\n\n\n5.6.2 Example\nIf we want to find the \\(z\\)-value corresponding to a probability of \\(0.475\\) between the mean and \\(z\\), we simply locate \\(0.475\\) (or the closest value) in the body of the standard normal table and then read off the corresponding \\(z\\)-score from the row and column headings. What do we get? \\(1.96\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#important-benchmark",
    "href": "Ch6.html#important-benchmark",
    "title": "5  The Normal Distribution",
    "section": "5.7 Important benchmark",
    "text": "5.7 Important benchmark\nApproximately 95% of the area under the normal curve lies between\n\\[\n-1.96 \\text{ and } +1.96.\n\\]\nWe often approximate this using \\(\\pm 2\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#example-heights",
    "href": "Ch6.html#example-heights",
    "title": "5  The Normal Distribution",
    "section": "5.8 Example: Heights",
    "text": "5.8 Example: Heights\nSuppose women’s heights have:\n\nmean \\(= 160\\) cm\n\nstandard deviation \\(= 5\\) cm\n\nWe want:\n\\[\nPr(155 \\le X \\le 170).\n\\]\nStandardize:\n\\[\nz = \\frac{X - 160}{5}.\n\\]\nSo:\n\n\\(155 \\rightarrow z = -1\\)\n\\(170 \\rightarrow z = 2\\)\n\nThus,\n\\[\nPr(-1 \\le Z \\le 2).\n\\]\nFrom the table:\n\\[\nPr(0 \\le Z \\le 1) = 0.3413,\n\\]\n\\[\nPr(0 \\le Z \\le 2) = 0.4772.\n\\]\nTherefore,\n\\[\nPr(-1 \\le Z \\le 2)\n= 0.3413 + 0.4772\n= 0.8185.\n\\]\nSo approximately 82% of women fall in that height range.\n\n\n\n\n\n\n\nImportantChapter summary\n\n\n\nThe normal distribution plays a central role because:\n\nmany natural phenomena are approximately normal,\nsums and averages tend toward normality,\nstandardization allows universal probability tables.\n\nThe \\(z\\)-score is the key tool for translating real-world measurements into probabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#conceptual-questions",
    "href": "Ch6.html#conceptual-questions",
    "title": "5  The Normal Distribution",
    "section": "6.1 Conceptual questions",
    "text": "6.1 Conceptual questions\n\nWhy does the Central Limit Theorem make the normal distribution especially important?\nExplain in words what the parameters \\(\\mu\\) and \\(\\sigma^2\\) represent.\nWhy must the total area under a probability density curve equal 1?\nWhat does it mean for a distribution to be symmetric?\nWhy is standardization useful?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#empirical-rule-questions",
    "href": "Ch6.html#empirical-rule-questions",
    "title": "5  The Normal Distribution",
    "section": "6.2 Empirical rule questions",
    "text": "6.2 Empirical rule questions\n\nIf a normal distribution has mean 100 and SD 10, what proportion of observations lie between 80 and 120?\nWhat proportion lies above 130?\nApproximately what percentage lies below \\(\\mu - 3\\sigma\\)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#standardization-and-table-exercises",
    "href": "Ch6.html#standardization-and-table-exercises",
    "title": "5  The Normal Distribution",
    "section": "6.3 Standardization and table exercises",
    "text": "6.3 Standardization and table exercises\n\nSuppose \\(X \\sim N(50, 9)\\). Compute:\n\n\\(Pr(X \\le 53)\\)\n\n\\(Pr(47 \\le X \\le 56)\\)\n\nSuppose \\(X \\sim N(100, 16)\\). Find \\(z\\) when \\(X = 104\\).\nFind \\(Pr(Z &gt; 1.64)\\).\nFind \\(z\\) such that:\n\n\\[\nPr(0 \\le Z \\le z) = 0.45.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#applied-problems",
    "href": "Ch6.html#applied-problems",
    "title": "5  The Normal Distribution",
    "section": "6.4 Applied problems",
    "text": "6.4 Applied problems\n\nTest scores are normally distributed with mean 70 and SD 8.\n\n\nWhat percentage score above 86?\n\nWhat percentage score between 62 and 78?\n\n\nIQ scores follow \\(N(100, 15^2)\\).\n\n\nWhat proportion have IQ above 130?\n\nWhat IQ corresponds to the top 5%?\n\n\nA factory produces rods with mean length 10 cm and SD 0.2 cm.\n\nWhat percentage are longer than 10.4 cm?\n\nIn Thailand, more than 1 million high school students took the university entrance exam in 1995. The average math score was 428 and the SD was 110.\n\n\nEstimate the 60th percentile of the math scores in 1995\nAmong Bangkok schools, the average math score was 448 with SD\nAbout what percent of the Bangkok high-school students did better than the national average?\n\n\nReplacement times for CD players are normally distributed with a mean of 7.1 years and a standard deviation of 1.4 years. If you want to provide a warranty so that only 2% of the CD players will be replaced before the warranty expires, what is the time length of the warranty\nMen have a sitting heights that are normally distributed with a mean of 36.0 in. and a standard deviation of 1.4 in. The Honda Civic has a headroom of 39 in. What is the probability that a randomly selected man will not fit in the Civic? If you want to design a car so that 97% of men can fit it in, what should the headroom be?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch6.html#challenge-question",
    "href": "Ch6.html#challenge-question",
    "title": "5  The Normal Distribution",
    "section": "6.5 Challenge question",
    "text": "6.5 Challenge question\n\nShow that standardizing a normal random variable produces a distribution with mean 0 and variance 1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Normal Distribution</span>"
    ]
  },
  {
    "objectID": "Ch7.html",
    "href": "Ch7.html",
    "title": "6  Elementary Probability",
    "section": "",
    "text": "6.1 The probability space, experiment, and outcome\nIn this chapter, we introduce the basic ideas of probability needed to understand statistical concepts and reasoning. The literature typically identifies three definitions of probability:\nThe classical definition assumes that all outcomes of an experiment are equally likely. For example, when rolling a fair die, the probability of obtaining an even number is the ratio of favorable outcomes (2, 4, 6) to total possible outcomes (1–6), which is \\(3/6 = 1/2\\).\nThe empirical (frequentist) definition is based on observed relative frequencies. For example, one might talk about the probability that Manchester United beats Chelsea, based on the fraction of past games won by Manchester United.\nThe subjective definition reflects an individual’s beliefs, informed by experience and available information. For instance, a student may assign a subjective probability to the event that they will earn an A in a statistics course like this one.\nConsider tossing a coin. The outcome is uncertain: it may land heads or tails. In probability language, tossing the coin is an experiment, and the result is an outcome.\nAn experiment produces exactly one outcome, chosen from a set of possible outcomes. This set is called the sample space, denoted \\(\\mathcal{S}\\). A subset of the sample space is called an event.\nThere is no restriction on what constitutes an experiment. Tossing one coin once, tossing it three times, or even tossing it infinitely many times can each be considered a single experiment.\nTo make this concrete, consider tossing a coin three times. The sample space is\n\\(\\mathcal{S} = \\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\}\\).\nIf we assume each outcome is equally likely, then each has probability \\(1/8\\).\nFrom this, it follows that for any event \\(A\\),\n\\[\nP(\\sim A) = 1 - P(A)\n\\tag{6.2}\\]\nwhere \\(\\sim A\\) denotes the complement of \\(A\\) or (not A).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#the-probability-space-experiment-and-outcome",
    "href": "Ch7.html#the-probability-space-experiment-and-outcome",
    "title": "6  Elementary Probability",
    "section": "",
    "text": "ImportantA basic fact\n\n\n\nProbabilities always lie between 0 and 1 (or 0% and 100%).\nMore formally:\n\\[\n0 \\leq Prob(A) \\leq 1\n\\tag{6.1}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#conditional-probability",
    "href": "Ch7.html#conditional-probability",
    "title": "6  Elementary Probability",
    "section": "6.2 Conditional probability",
    "text": "6.2 Conditional probability\nConditional probability is best understood through an example.\nSuppose a deck of cards is shuffled and two cards are drawn. You win 100 baht if the second card is the ACE of hearts. Before seeing any cards, the probability of winning is \\(1/52\\).\nNow imagine that the first card is revealed and it is the seven of clubs. What is your probability of winning now?\nSince one card is no longer in the deck (and you know what it is), the probability becomes \\(1/51\\).\nPut differentlay, when pulling out two cards, because you now know the first card, the probability of the second card is a conditional probability i.e. the probability of a particular event occurring, given that another event has occurred. This is compactly written as \\(P(A \\mid B)\\).\n\n\n\n\n\n\nImportantConditional probability\n\n\n\nConditional probability reflects how probabilities change once new information becomes available.\n\n\nFormally, the probability that event \\(A\\) occurs given that event \\(B\\) has occurred is written\n\\[\nP(A \\mid B)\n\\tag{6.3}\\]\nwhich is read as the probability of \\(A\\) given \\(B\\).\n\n\n\n\n\n\nTipTry This: Conditional Probability\n\n\n\n(1) Two tickets are drawn at random without replacement from the following box:\n\n\nWhat is the chance that the second ticket is \\(\\fbox{B}\\)?\nWhat is the chance that the second ticket is \\(\\fbox{B}\\), given thayt the fist is \\(\\fbox{A}\\)?\n\n(2) Three cards are drawn from a deck of cards without replacement. What is the probability that none of them will be hearts",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#the-multiplication-rule",
    "href": "Ch7.html#the-multiplication-rule",
    "title": "6  Elementary Probability",
    "section": "6.3 The multiplication rule",
    "text": "6.3 The multiplication rule\nConsider a box containing three tickets labeled R, W, and B. Two tickets are drawn without replacement. What is the probability of drawing R first and then W?\nOn the first draw, the probability of R is \\(1/3\\). Given that R was drawn, only W and B remain, so the probability of W on the second draw is \\(1/2\\). Therefore,\n\\(P(R \\text{ then } W) = \\displaystyle \\frac{1}{2} \\text{ of } \\frac{1}{3} = \\frac{1}{6}\\).\nThis illustrates the multiplication rule:\n\\[\nP(AB) = P(B \\mid A)P(A) = P(A \\mid B)P(B)\n\\tag{6.4}\\]\nFor example, the probability that two cards dealt from a deck are both ACES is\n\\(\\displaystyle \\frac{3}{51} \\times \\frac{4}{54}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#independence",
    "href": "Ch7.html#independence",
    "title": "6  Elementary Probability",
    "section": "6.4 Independence",
    "text": "6.4 Independence\nTwo events are said to be independent if knowing the outcome of one does not affect the probability of the other.\nDrawing with replacement produces independent events; drawing without replacement produces dependent events.\n\n\n\n\n\n\nImportantIndependence\n\n\n\nEvents A and B as said to be independent if\n\\[\nP(A \\mid B) = P(A)\n\\tag{6.5}\\]\n(or equivalently \\(P(B \\mid A) = P(B)\\)).\n\n\nWhen we have independence, the multiplication rule conveniently simplifies to\n\\[\nP(AB) = P(A)P(B)\n\\tag{6.6}\\]\nTry these: (1) A fair coin is tossed twice. What is the chance of getting a head then a tail? (2) A die is rolled three times. (i) Find the chance that the first roll is an ACE. (ii) Find the chance that the first roll is an ACE, the second roll is a deuce, and the third roll is a trey..",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#checking-for-independence",
    "href": "Ch7.html#checking-for-independence",
    "title": "6  Elementary Probability",
    "section": "6.5 Checking for independence",
    "text": "6.5 Checking for independence\nSuppose we want to know whether graduating from GA program helps someone become a successful manager/CEO.\nLet:\n\n\\(A_i\\): GA graduate (=1), or 0 otherwise\n\n\\(B_i\\): Becomes CEO (=1), or 0 otherwise\n\nThe table below summarizes survey results:\n\n\n\n\nBecomes CEO\nDoes not become CEO\nTotal\n\n\n\n\nGA graduate\n0.11\n0.29\n0.40\n\n\nNot GA graduate\n0.06\n0.54\n0.60\n\n\nTotal\n0.17\n0.83\n1.00\n\n\n\nThe marginal probability of becoming a CEO is 0.17. The joint probability of a GA graduate becoming a CEO is 0.11, and so on.\n\n\n\n\n\n\nImportantMarginal probability from joint probabilities\n\n\n\nTo get the marginal probability of a student being a GA graduate, we add the two joint probabilities of the first row \\((0.11 + 029)\\) to get \\(0.4\\). Note here the two jint probabilities that we add, the probability that we have a GA graduate and CEO plus the probability that we have a GA graduate and not a CEO. Here probabilty of GA is the same, but we have CEO and not CEO. So by adding across CEO type, we essentially integrate it out and we are left with the marginal probabilty of 0.4, the probability of GA graduate.\n\n\nTo test independence, compute\n\\(P(A_1 \\mid B_1) = \\displaystyle \\frac{P(A_1 B_1)}{P(B_1)} = \\displaystyle \\frac{0.11}{0.17} \\approx 0.65\\).\nSince \\(P(A_1 \\mid B_1) \\neq P(A_1)\\), the two events are not independent.\n\n\n\n\n\n\nTipInterpretation\n\n\n\nBecoming a CEO is related to being a GA graduate in this example.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#the-addition-rule",
    "href": "Ch7.html#the-addition-rule",
    "title": "6  Elementary Probability",
    "section": "6.6 The addition rule",
    "text": "6.6 The addition rule\nTwo events are said to be mutually exclusive if the occurrence of one prevents the occurrence of the other.\nFor mutually exclusive events,\n\\[\nP(A \\text{ or } B) = P(A) + P(B)\n\\tag{6.7}\\]\nFor example, intuitively, if we draw one card from a deck of cards, the probability of getting either a heart or spades is \\(\\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\\). The additional rule tells us to simply add the chances provided that the two events are mutually exclusive. The probability to get a heart is 1/4 and the probability to get a spade is also 1/4. Are the two events mutually exclusive? Yes, getting a heart means you can’t get a spade, and vice versa. So simply add the chances to get \\(1/4 + 1/4 = 1/2.\\)\nHowever, for events that are not mutually exclusive, the general addition rule is\n\\[\nP(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)\n\\tag{6.8}\\]\nFor example, if we toss two dice at the same time, to get at least ace on the two dice, we calculate the probability using the general additional rule, i.e we add the chances and subtract the joint probability because the two events are not mutually exclusive since having an ace on one of the die does not exclude the possibility to get an ace on the other die. In other words, it is possible to get ACE in both dice, i.e. the events are not mutually exclusive! Hence we get \\(1/6 + 1/6 - 1/36 = 11/36.\\)\n\n\n\n\n\n\nImportantMutually exclusive events\n\n\n\nEvents A and B as said to be mutually exclusive if\n\\[\nP(A \\text{ and } B) = 0\n\\tag{6.9}\\]\n\n\nNote that we have used the word ‘or’ and ‘at least’ for the additional rule, which can be contrasted with ‘and’ for the multiplication rule. In terms of set operations, these are the union and intersect operators, respectively.\n\n\n\n\n\n\nImportantTry these.\n\n\n\n(1) A die is rolled twice. What is (a) the chance that the first roll is an ace or the second roll is an ace? (b) the chance that the first roll is an ace and the second roll is an ace? (c) the cace of getting at least one ace?\n(2) A deck of cards is shuffled. What is (a) the chance that the top card is an ace of spades or the bottom card is an ace of spades? (b) the chance that the first card is an ace of spades and the bottom card is an ace of spades?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#venn-diagrams",
    "href": "Ch7.html#venn-diagrams",
    "title": "6  Elementary Probability",
    "section": "6.7 Venn diagrams",
    "text": "6.7 Venn diagrams\nVenn diagrams are very useful when thinking about simple probability problems.\nFor example, two events A and B that are not disjoint can be drawn as follows:\n\n\n\n\n\nEven more useful, the conditional probability can be shown by the following:\n\\(P(A|B) =\\)\n or \\(\\frac{P(AB)}{P(B)}.\\) That is, given B we wish to know A.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#bayes-rule",
    "href": "Ch7.html#bayes-rule",
    "title": "6  Elementary Probability",
    "section": "6.8 Bayes’ rule",
    "text": "6.8 Bayes’ rule\nWe can easily derive the Bayes’ Rule using Venn diagrams.\nTaking a sample space with event A:\n\nSimilarly, the probability of B can be shown using the Venn diagram as follows: \\\n\nCombining the two Venn diagrams we get:\\\n\nOr what is the same thing\n\\[\nP(A|B) = \\frac {P(A \\text{ and } B)}{P(B)} = \\frac {P(A \\text{ and } B)}{P(A \\text{ and } B) + P(\\sim A \\text{ and } B)}\n\\]\nAnd finally from the definition of joint probabilities we have the Bayes Rule.\n\n\n\n\n\n\nImportantBayes’ Rule\n\n\n\n\\[\nP(A \\mid B) = \\frac{P(A)P(B \\mid A)}{P(A)P(B \\mid A) + P(\\sim A)P(B \\mid \\sim A)}\n\\tag{6.10}\\]\n\n\nSo how do we use the Bayes’ Rule? An example should illustrate this. Assume that on a dark night, there was a hit and run accident. A witness, who says that she saw a blue taxi, agrees to sit in the court to testify for the victim so that the latter might get some compensation from the blue taxi company. Incidentally, there are 200 taxis in this town; 170 or 85 % belong to the black taxi company and 30 or 15 % are blue. According to tests conducted under the same condition, the witness identifies blue taxi about 80 % of the time. Now, the question is, What is the probability that she was right, that she really did see a blue taxi that night?\n\n15% of taxis are blue\n\n85% are black\n\nThe witness correctly identifies blue taxis 80% of the time\n\nShe incorrectly identifies black taxis as blue 20% of the time\n\nTo use Bayes’ formula, all we have to identify are a few marginal and conditional probabilities; \\(P(A)\\) is the probability that the taxi involved in the accident is blue (i.e 15 %.) Hence \\(P(\\sim A)\\) that is the probability of a black taxi, is 85 %. \\(P(B \\mid A)\\) is the probability that the witness claims to have seen blue when a blue taxi was really involved, in other words, her accuracy in identifying a blue taxi correctly under the conditions. Conversely, the only other probability we need now to complete the equation is \\(P(B \\mid \\sim A)\\), which is the probability that the witness sees a blue taxi when in fact a black taxi was involved. i.e 20 %. Plugging all the relevant probabilities gives:\n\\(P(A \\mid B) = \\displaystyle \\frac{0.15 \\times 0.80}{(0.15 \\times 0.80) + (0.85 \\times 0.20)} = \\frac{0.12}{0.29} \\approx 0.41\\).\nThe probability that the witness says she saw “blue” and that it was really a blue taxi is only 0.41, hardly sufficient for the courts to ask the blue taxi company to pay compensation!1\n\n\n\n\n\n\nWarningA sobering result\n\n\n\nEven confident testimony can be misleading when base rates are ignored.\n\n\n\n\n\n\n\n\nTipProbability tree diagram\n\n\n\nWe can represent the example in a tree diagram.\n                            Start: Taxi Accident\n                                   |\n               ┌───────────────────┴───────────────────┐\n               │                                       │\n           Blue Taxi (A)                         Black Taxi (~A)\n           P(A) = 0.15                          P(~A) = 0.85\n               |                                       |\n        ┌──────┴───────┐                       ┌──────┴───────┐\n        │              │                       │              │\n  Says Blue (B)   Says Black (~B)        Says Blue (B)   Says Black (~B)\n  P(B|A)=0.80     P(~B|A)=0.20          P(B|~A)=0.20     P(~B|~A)=0.80\n        |              |                       |              |\n        ▼              ▼                       ▼              ▼\n  Joint: 0.12     Joint: 0.03           Joint: 0.17      Joint: 0.68\n  P(A∩B)          P(A∩~B)               P(~A∩B)          P(~A∩~B)\nWe see that:\n\\(P(AB) = 0.12\\)\n\\(P(B) = P(A∩B) + P(\\sim A∩B) = 0.12 + 0.17 = 0.29\\) i.e. the marginal probability of \\(B\\) by integrating out \\(A\\) by summing across \\(A\\).\nAnd since we are looking for \\(P(A|B)\\) i.e. \\(P(AB)/P(B) ≈ 0.41\\), which is the probability the taxi was blue given that the witness says blue!\n\n\n\n6.8.0.1 Bayes’ Updating\nEssentially, we have:\n\n\n\n\n\nPrior Probability (15%): This represents our initial belief about the situation before considering the witness testimony. Based on the town’s taxi distribution, we know that only 15% of taxis are blue. This is our baseline expectation that any random taxi involved in an accident would be blue.\nLikelihood (Witness Accuracy): This quantifies how probable the observed evidence (witness testimony) is under different hypotheses. The witness correctly identifies blue taxis 80% of the time and incorrectly identifies black taxis as blue 20% of the time. This creates two competing likelihoods: - If the taxi was actually blue, we’d expect her to say “blue” 80% of the time - If the taxi was actually black, we’d expect her to mistakenly say “blue” 20% of the time\nPosterior Probability (41%): This is the updated probability that combines our prior belief with the new evidence. Using Bayes’ theorem, we find that even though the witness says she saw a blue taxi, there’s only about a 41% chance she’s actually correct. This counterintuitive result occurs because blue taxis are rare (15%) and the false positives from the much more common black taxis outweigh the true positives.\nIntuition Behind the Result: The Bayesian approach forces us to consider both the reliability of the evidence AND the base rates (or prioir provbailities). Even with a seemingly reliable witness (80% accurate), the rarity of blue taxis means most “blue” identifications will be errors. This explains why the court should be skeptical—the witness’s claim only increases the probability from 15% to 41%, far from “beyond reasonable doubt.”\nExtension: What If We Had Another Witness? If a second independent witness also claimed to see a blue taxi, Bayesian updating would apply again. The first posterior (41%) would become our new prior, and we’d combine it with the second witness’s reliability. Intuitively, this would increase our confidence, but mathematically, we’d need to consider (1) whether the witnesses are truly independent, (2) whether they have the same accuracy rates, and (3) the possibility of collusion or shared biases.\nWith two independent, reliable witnesses both saying “blue,” the probability that the taxi was actually blue would increase substantially, potentially making the evidence strong enough for legal purposes. This demonstrates how Bayesian updating elegantly handles accumulating evidence—each new piece of information updates our beliefs in a mathematically rigorous way.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#axioms-of-probability",
    "href": "Ch7.html#axioms-of-probability",
    "title": "6  Elementary Probability",
    "section": "6.9 Axioms of probability",
    "text": "6.9 Axioms of probability\nProbability rests on three axioms:\n\nNonnegativity: \\(P(A) \\ge 0\\)\n\nAdditivity: For disjoint events, \\(P(A \\cup B) = P(A) + P(B)\\)\n\nNormalization: \\(P(\\mathcal{S}) = 1\\)\n\nFrom these axioms follow many useful properties, such as:\n\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\\(P(A \\cup B) \\le P(A) + P(B)\\)\n\n\n\n\n\n\n\nImportantChapter summary\n\n\n\nProbability provides a formal language for reasoning under uncertainty. In this chapter, we introduced experiments, events, conditional probability, independence, and the fundamental rules governing probabilities. These ideas form the foundation for statistical inference in the chapters ahead.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#exercises",
    "href": "Ch7.html#exercises",
    "title": "6  Elementary Probability",
    "section": "6.10 Exercises",
    "text": "6.10 Exercises\n\n6.10.1 Conceptual questions\n\nThree views of probability\nExplain the difference between the classical, frequentist, and subjective interpretations of probability.\nGive one real-world example for each interpretation.\nProbability without philosophy\nThe chapter argues that the mathematics of probability does not depend on which interpretation we adopt.\nExplain what this means in your own words.\nExperiments and outcomes\nConsider the experiment “toss a fair coin until a head appears.”\n\nWhat is the sample space?\n\nIs this a finite or infinite sample space?\n\nWhy is this still considered a single experiment?\n\nEvents vs outcomes\nExplain the difference between an outcome and an event.\nCan a single outcome itself be an event?\nConditional probability and information\nWhy does conditional probability capture the idea of learning from new information?\nGive an example (not involving cards or coins) where learning new information changes probabilities.\nIndependence vs mutual exclusivity\nAre mutually exclusive events ever independent?\nCarefully explain your answer.\n\n\n\n\n6.10.2 Basic probability calculations\n\nCoins and sample spaces\nA fair coin is tossed four times.\n\nHow many outcomes are in the sample space?\n\nWhat is the probability of getting exactly two heads?\n\nWhat is the probability of getting at least one head?\n\nComplements\nLet event \\(A\\) be “a randomly selected student passes an exam”, with \\(P(A)=0.85\\).\n\nWhat is \\(P(\\sim A)\\)?\n\nInterpret this probability in words.\n\nCards and conditional probability\nA card is drawn at random from a standard deck.\n\nWhat is the probability that it is a queen?\n\nWhat is the probability that it is a queen given that it is a face card?\n\nAre the events “queen” and “face card” independent?\n\n\n\n\n\n6.10.3 Multiplication rule and independence\n\nWithout replacement\nTwo cards are drawn without replacement from a standard deck.\n\nWhat is the probability that both cards are red?\n\nWhat is the probability that the first card is an ace and the second card is also an ace?\n\nWith replacement\nRepeat Question 10, but now assume the first card is replaced before the second draw.\nExplain why the probabilities change.\nTesting independence\nTwo events satisfy\n\\(P(A)=0.4\\), \\(P(B)=0.5\\), and \\(P(AB)=0.2\\).\n\nAre \\(A\\) and \\(B\\) independent?\n\nJustify your answer mathematically.\n\n\n\n\n\n6.10.4 Addition rule and mutually exclusive events\n\nMutually exclusive or not?\nFor each pair of events below, state whether they are mutually exclusive and explain why:\n\nDrawing a heart and drawing a spade\n\nDrawing a heart and drawing a red card\n\nGetting a head on the first toss and a tail on the second toss\n\nDice example\nTwo fair dice are rolled.\n\nWhat is the probability that at least one die shows a 6?\n\nUse the general addition rule to compute your answer.\n\n\n\n\n\n6.10.5 Bayes’ rule and interpretation\n\nMedical testing\nA disease affects 2% of a population.\nA diagnostic test correctly identifies the disease 95% of the time, but incorrectly signals disease in 5% of healthy individuals.\n\nDefine the relevant events.\n\nUse Bayes’ rule to compute the probability that a person who tests positive actually has the disease.\n\nExplain the result in words.\n\nBase-rate neglect\nIn the taxi example discussed in the chapter, many people expect the probability to be close to 0.8.\nExplain why this intuition is misleading.\n\n\n\n\n6.10.6 Set Operations and Probability Identities\n\nSet operations and De Morgan’s laws\n\n\n\nConsider rolling a fair die. Let\n\n\n\n\\(A\\) be the set of outcomes where the roll is an even number, and\n\n\\(B\\) be the set of outcomes where the roll is greater than 3.\n\nCalculate and compare the sets on both sides of De Morgan’s laws:\n\\[\n\\sim (A \\cup B) = \\sim A \\cap \\sim B\n\\qquad \\text{and} \\qquad\n\\sim (A \\cap B) = \\sim A \\cup \\sim B.\n\\]\n\nShow that\n\n\\[\n\\sim A = (\\sim A \\cap B) \\cup (\\sim A \\cap \\sim B),\n\\]\nand\n\\[\n\\sim B = (A \\cap \\sim B) \\cup (\\sim A \\cap \\sim B).\n\\]\n\nShow that\n\n\\[\n\\sim (A \\cap B)\n= (\\sim A \\cap B) \\cup (\\sim A \\cap \\sim B) \\cup (A \\cap \\sim B).\n\\]\n\nProbability inequality\n\nProve that for any two events \\(A\\) and \\(B\\),\n\\[\nP(A \\cap B) \\ge P(A) + P(B) - 1.\n\\]\nHint: Use the axioms of probability and the addition rule.\n\nLaw of total probability\n\nShow the identity\n\\[\nP(A \\mid B)\n= P(C \\mid B)P(A \\mid B \\cap C)\n+ P(\\sim C \\mid B)P(A \\mid B \\cap \\sim C).\n\\]\nExplain briefly how this formula relates to conditioning on whether event \\(C\\) occurs or not.\n\n\n\n6.10.7 Reflection and interpretation\n\nProbability and statistical reasoning\nExplain why probability theory is essential for understanding statistical concepts such as correlation and regression.\nFrom probability to inference\nWhich ideas from this chapter do you think will be most important when we later study statistical inference?\nBriefly explain why.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch7.html#footnotes",
    "href": "Ch7.html#footnotes",
    "title": "6  Elementary Probability",
    "section": "",
    "text": "Another way to see this is, say we present the witness the 30 Blue taxis, she will identify 80% or 24 as blue (and 6 as black) under the same night conditions. Also she will identify 20% or 34 of 170 black taxis as ‘blue’. So out of 54 taxis she calls ‘blue’, only 24 are in fact blue, which is about 41%.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Elementary Probability</span>"
    ]
  },
  {
    "objectID": "Ch8.html",
    "href": "Ch8.html",
    "title": "7  Discrete Probability Distribution",
    "section": "",
    "text": "7.1 Probability distributions\nWe begin by defining a random variable as a rule that assigns a number to each outcome of an experiment.\nInstead of describing a single coin toss as \\(\\{\\text{Heads}, \\text{Tails}\\}\\), for example, we can “translate” outcomes into numbers such as 1 and 0. That numerical description is often more convenient — because once outcomes are numbers, we can compute averages, spreads, and relationships.\nSo a random variable is a bridge:\nexperiment \\(\\rightarrow\\) outcomes \\(\\rightarrow\\) numbers\nOnce we have a random variable \\(X\\), we can ask:\nHow likely is each possible value of \\(X\\)?\nA probability distribution answers this question by listing the possible values of \\(X\\) and the probability attached to each one.\nFormally, if \\(A\\) is a set of real numbers, the probability that \\(X\\) falls in \\(A\\) is\n\\(P_X(A) = P(\\{\\omega \\in \\Omega : X(\\omega)\\in A\\})\\).\nYou do not need to memorize that expression. The idea is simple:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#probability-distributions",
    "href": "Ch8.html#probability-distributions",
    "title": "7  Discrete Probability Distribution",
    "section": "",
    "text": "The probability that \\(X\\) lands in a set \\(A\\) equals the probability of all outcomes that map into \\(A\\).\n\n\n\n\n\n\n\nTipTossing a fair coin (as a random variable)\n\n\n\n\nSample space: \\(\\Omega = \\{\\text{Heads}, \\text{Tails}\\}\\)\n\nRandom variable: \\(X(\\text{Heads}) = 1\\), \\(X(\\text{Tails}) = 0\\)\n\nDistribution: \\(P(X=1)=1/2\\), \\(P(X=0)=1/2\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#discrete-vs.-continuous-random-variables",
    "href": "Ch8.html#discrete-vs.-continuous-random-variables",
    "title": "7  Discrete Probability Distribution",
    "section": "7.2 Discrete vs. continuous random variables",
    "text": "7.2 Discrete vs. continuous random variables\nA discrete random variable takes values that are separated and countable (often whole numbers like \\(0,1,2,\\dots\\)).\nA continuous random variable can take any value on an interval of the real line.\nA helpful analogy:\n\nIntegers are discrete\nReal numbers are continuous\n\n\n\n\n\n\n\nImportantDiscrete vs. continuous\n\n\n\n\nDiscrete random variable: described by a probability mass function (PMF)\n\n\\(P(X=x_i)=p_i,\\quad p_i\\ge 0,\\quad \\sum_i p_i = 1\\)\n\nContinuous random variable: described by a probability density function (PDF)\n\n\\(P(a \\le X \\le b)=\\int_a^b f(x)\\,dx,\\quad f(x)\\ge 0,\\quad \\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#the-discrete-random-variable-an-example",
    "href": "Ch8.html#the-discrete-random-variable-an-example",
    "title": "7  Discrete Probability Distribution",
    "section": "7.3 The discrete random variable: an example",
    "text": "7.3 The discrete random variable: an example\nIn earlier chapters, we used histograms, means, and standard deviations to summarize data.\nIn probability, we summarize random variables — and the key object is the probability distribution, which tells us how probabilities are allocated across possible outcomes.\n\n7.3.1 Example: tossing a coin three times\nConsider the experiment: toss a coin three times.\nLet \\(X\\) be the random variable “number of heads in 3 tosses.”\nThe sample space has 8 equally likely outcomes. We can list outcomes and compute \\(X\\):\n\n\n\nTable 7.1\n\n\n\\[\n\\begin{array}{|c|c|}\n\\hline\n\\text{Outcome} & X = \\text{No. of Heads} \\\\\n\\hline\nTTT & 0 \\\\\nTTH & 1 \\\\\nTHT & 1 \\\\\nTHH & 2 \\\\\nHTT & 1 \\\\\nHHT & 2 \\\\\nHTH & 2 \\\\\nHHH & 3 \\\\\n\\hline\n\\end{array}\n\\] Outcomes of 3 tosses of a coin\n\n\n\nFrom this we get the PMF (probability distribution) of \\(X\\):\n\n\n\nTable 7.2\n\n\n\\[\n\\begin{array}{|c|c|}\n\\hline\nx & P(X=x) \\\\\n\\hline\n0 & 1/8 = 0.125 \\\\\n1 & 3/8 = 0.375 \\\\\n2 & 3/8 = 0.375 \\\\\n3 & 1/8 = 0.125 \\\\\n\\hline\n\\Sigma & 1.000 \\\\\n\\hline\n\\end{array}\n\\] Probability distribution of \\(X\\) = number of heads in 3 tosses",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#visualizing-the-pmf-and-cdf",
    "href": "Ch8.html#visualizing-the-pmf-and-cdf",
    "title": "7  Discrete Probability Distribution",
    "section": "7.4 Visualizing the PMF and CDF",
    "text": "7.4 Visualizing the PMF and CDF\n\n7.4.1 PMF (a probability “histogram”)\n\n\n\n\n\n\n\n\n\n\n\n7.4.2 CDF (cumulative distribution function)\nThe CDF tells us \\(P(X\\le x)\\) — the probability that the random variable is at most \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#the-mean-and-the-variance",
    "href": "Ch8.html#the-mean-and-the-variance",
    "title": "7  Discrete Probability Distribution",
    "section": "7.5 The mean and the variance",
    "text": "7.5 The mean and the variance\nApart from drawing the probability distribution or histograms, we have seen that numerical summaries such as the mean and variance are useful.\nThe mean is the long-run average value of a random variable, which is also referred to as its expected value, denoted \\(E(X)\\).1\n\\[\nE(X) = \\mu = \\sum_{i=1}^{n} P(X_i) X_i\n\\tag{7.1}\\]\nThe variance which measures the spread or variation of the distribution is defined as:\n\\[\nVar(X) = \\sigma^2 = \\sum_{i=1}^{n} P(X_i) (X_i- {\\overline X})^2\n\\tag{7.2}\\]\nTo get the standard deviation, simply calculate the square root of the variance.\nIt is easy to see that the formulas for expected value and variance are structurally identical—they are both weighted averages–to the “usual” formulas for mean and variance in previous chapters. The only difference is what weights are used.\n\n\n\n\n\n\nNoteA key viewpoint\n\n\n\nBoth \\(E(X)\\) and \\(\\mathrm{Var}(X)\\) are weighted averages.\nIn samples, each observation often has equal weight \\(1/n\\).\nIn probability models, values are weighted by their probabilities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#example",
    "href": "Ch8.html#example",
    "title": "7  Discrete Probability Distribution",
    "section": "7.6 Example",
    "text": "7.6 Example\nLet’s apply the expected value and variance formulas with the example above which defines the random variable \\(X\\) as the number of heads in 3 tosses.\nTo compute the mean, use Equation 7.1, shown in the table below–summing the last column of the table gives \\(\\mu = 1.5\\).\n\n\n\nTable 7.3: Calculating the mean.\n\n\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\nX & P(X) & X \\cdot P(X) \\\\\n\\hline\n0 & 1/8 & 0 \\\\\n1 & 3/8 & 3/8 \\\\\n2 & 3/8 & 6/8 \\\\\n3 & 1/8 & 3/8 \\\\\n\\hline\n\\textbf{Total} & \\mathbf{1} & \\mathbf{12/8 = 1.5} \\\\\n\\hline\n\\end{array}\n\\]\n\n\n\nTo get the variance, applying Equation 7.2, as shown in the the table below and summing the last column of the table gives \\(\\sigma^2 = 0.75\\)\n\n\n\nTable 7.4: Calculating the variance.\n\n\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\nX & P(X) & (X_i - \\mu) & (X_i - \\mu)^2 \\cdot P(X) \\\\\n\\hline\n0 & 0.125 & -1.5 & 0.28125 \\\\\n1 & 0.375 & -0.5 & 0.09375 \\\\\n2 & 0.375 & 0.5 & 0.09375 \\\\\n3 & 0.125 & 1.5 & 0.28125 \\\\\n\\hline\n\\mathbf{\\Sigma} & \\mathbf{1.000} & & \\sigma^2 = \\mathbf{0.75} \\\\\n\\hline\n\\end{array}\n\\]\n\n\n\nCareful, we have the variance \\(=0.75\\), hence the standard deviation \\(\\sigma = \\sqrt{0.75}\\) which is about \\(0.8667\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#rules-of-expected-value-and-variance",
    "href": "Ch8.html#rules-of-expected-value-and-variance",
    "title": "7  Discrete Probability Distribution",
    "section": "7.7 Rules of expected value and variance",
    "text": "7.7 Rules of expected value and variance\nLet us introduce some basic rules of expected value and variance. Firstly, for expected value, we have:\nRule E1 \\[E(k)=k\\] Rule E2 \\[E(X+k)=E(X)+k\\]\nRule E3 \\[E(kX)=kE(X)\\]\nRule E1 states that the expected value (or average) of a constant is that constant. Rule E2 states that the expected value of a random variable to which a constant has been added is equal to the expected value of the random variable plus the constant. Rule E3 states that the expected value of a random variable multiplied by a constant is equal to the constant times the expected value of the random variable.\nSecondly, the rules of variance are:\nRule V1 \\[Var(k)=0\\]\nRule V2 \\[Var(X+k)=Var(X)\\]\nRule V3 \\[Var(kX)=k^2 Var(X)\\]\nRule V1 states that the variance of a constant is zero (i.e., there is no spread). Rule V2 states that the variance of a random variable to which a constant has been added is simply equal to the variance of the random variable. Lastly, Rule V3 states that the variance of a random variable multiplied by a constant is equal to the constant squared times the variance of the random variable.2\n\n\n\n\n\n\nNoteExample: Monthly Profit Analysis\n\n\n\nLet’s illustrate how these rules apply to a real-world scenario.\nThe Data: For a food store, monthly sales have a mean (\\(\\mu\\)) of Baht 25,000 and a standard deviation (\\(\\sigma\\)) of Baht 4,000. Profits are defined as 30% of sales minus fixed costs of Baht 6,000.\nThe Goal: Find the mean and standard deviation of the monthly profit.\n1. Define the Equation: \\[Profit = 0.30(Sales) - 6,000\\]\n2. Calculate the Expected Value (Mean):\n\\[\n\\begin{aligned}\nV(\\text{Profit}) &= V[0.30(\\text{Sales}) - 6,000] \\\\\n                 &= V[0.30(\\text{Sales})]           && \\text{(Rules V2 \\& V1)} \\\\\n                 &= (0.30)^2 V(\\text{Sales})        && \\text{(Rule V3)} \\\\\n                 &= 0.09 \\cdot (4,000)^2            \\\\\n                 &= 0.09 \\cdot 16,000,000           \\\\\n                 &= 1,440,000\n\\end{aligned}\n\\]\n3. Calculate the Variance and Standard Deviation: First, we find the variance (\\(V\\)). Remember that \\(V(Sales) = \\sigma^2 = 4,000^2\\). \\[\n\\begin{aligned}\nV(\\text{Profit}) &= V[0.30(\\text{Sales}) - 6,000] \\\\\n                 &= V[0.30(\\text{Sales})]           && \\text{(Rules V2 \\& V1)} \\\\\n                 &= (0.30)^2 V(\\text{Sales})        && \\text{(Rule V3)} \\\\\n                 &= 0.09 \\cdot (4,000)^2            \\\\\n                 &= 0.09 \\cdot 16,000,000           \\\\\n                 &= 1,440,000\n\\end{aligned}\n\\]\nFinally, the standard deviation is: \\[\\sigma_{\\text{Profit}} = \\sqrt{1,440,000} = \\mathbf{1,200}\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#discrete-bivariate-distributions",
    "href": "Ch8.html#discrete-bivariate-distributions",
    "title": "7  Discrete Probability Distribution",
    "section": "7.8 Discrete bivariate distributions",
    "text": "7.8 Discrete bivariate distributions\nSo far, we have focused on univariate distributions. We now extend these ideas to bivariate probability distributions, which arise from joint probabilities.\nA joint probability distribution of two discrete random variables \\(X\\) and \\(Y\\) is a table or formula that lists the joint probabilities for all pairs of values \\((x, y)\\). It is denoted by \\(P(X,Y)\\) or, equivalently, \\(P(X = x \\text{ and } Y = y)\\).\nAs we would expect, joint probabilities satisfy the following basic properties:\n\\(0 \\le P(X,Y) \\le 1\\)\nand\n\\(\\sum_x \\sum_y P(X,Y) = 1\\).\n\n7.8.1 Joint and marginal probabilities\nLet’s take an example. Xavier and Yvette are real estate agents; Let’s use X and Y to denote the number of houses each sells in a month, respectively. The following joint probabilities are based on past sales performance.\n\n\n\n\nx=0\nx=1\nx=2\n\n\n\n\nY=0\n0.12\n0.42\n0.06\n\n\nY=1\n0.21\n0.06\n0.03\n\n\nY=2\n0.07\n0.02\n0.01\n\n\n\n\n7.8.1.1 Marginal distribution\nThe marginal probabilities are obtained by summing the joint probabilities across rows or down the columns. They describe the probabilities of \\(Y\\) and \\(X\\) individually, ignoring the other variable.\n\n\n\n\n\n\nFigure 7.1: Marginal Distribution\n\n\n\nThen using these marginal probabilities, we can compute the mean, and variance of each variable in a bivariate distribution by Equation 7.1 and Equation 7.2.\nCheck that you get \\(E(X)=0.7\\) and \\(E(Y)=0.6\\), respectively. How about the variances and standard deviations?3\n\n\n\n\n7.8.2 Correlation\nThe correlation coefficient is obtained by dividing the covariance by the product of the standard deviations:\n\\[\n\\text{Corr}(X,Y) = \\dfrac{\\text{Covariance}(X,Y)}{SD_X \\, SD_Y}\n\\]\nLet’s try finding the correlation of Xavier and Yvette.\n\n7.8.2.1 Covariance of two discrete variables\nStarting from the covariance between two discrete random variables defined as\n\\[\nCov(X,Y) = P(X,Y) \\sum_X \\sum_Y (X - \\overline{X})(Y - \\overline{Y})\n\\tag{7.3}\\]\nor an alternative shorter formula is: \\[\nCov(X, Y) = \\sum_x \\sum_y XY P(X, Y) - \\overline{XY}\n\\]\nTry showing that the two equations are in fact the same.\nOperationalizing Equation 7.3 gives\n\\[\n\\text{Covariance}(X,Y) = (0–.7)(0–.5)(.12) + (1–.7)(0-.5)(.42) + \\dots + (2–.7)(2–.5)(.01) = –0.15\n\\]\n\n\n\n\n\n\nNoteCovariance and correlation\n\n\n\nCovariance measures whether \\(X\\) and \\(Y\\) tend to move together, while the correlation coefficient rescales covariance into a unit-free number:\n\\[\n\\mathrm{Corr}(X,Y)=\\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{SD}_X \\mathrm{SD}_Y}\n\\].\nCorrelation is always between \\(-1\\) and \\(1\\).\n\n\nAnd finally, we get:\n\\[\n\\text{Corr} (X,Y) = \\dfrac{-0.15}{0.64 \\times 0.67} \\approx -0.35\n\\]\nA negative correlation meaning when Xavier does well, Yvette doesn’t (and vice-versa).\n\n\n\n\n\n\n\nImportantChapter Summary\n\n\n\nIn this chapter, we introduced a random variable as a rule that assigns numbers to the outcomes of a random experiment, allowing us to analyze uncertainty mathematically.\nA discrete probability distribution describes how probabilities are assigned to distinct values through a probability mass function (PMF). These probabilities must be nonnegative and sum to one.\nWe defined the expected value and variance as probability-weighted summaries of the distribution. The expected value represents the long-run average, while the variance measures dispersion around the mean.\nExtending these ideas to two variables, we studied joint distributions, from which marginal distributions can be derived. Finally, we introduced covariance and correlation to measure the direction and strength of linear association between discrete random variables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#exercises",
    "href": "Ch8.html#exercises",
    "title": "7  Discrete Probability Distribution",
    "section": "7.9 Exercises",
    "text": "7.9 Exercises\n\n7.9.1 Understanding discrete random variables\n\nRandom variables\n\n\nWhat is a discrete random variable?\n\nExplain the difference between a random outcome and a random variable.\n\nGive two real-world examples of discrete random variables.\n\n\nProbability distributions\n\nA discrete random variable \\(X\\) takes values \\(x_1, x_2, \\dots, x_k\\) with probabilities \\(P(X=x_i)\\).\n\nState the two conditions a probability distribution must satisfy.\n\nExplain in words why these conditions are necessary.\n\n\n\n\n7.9.2 Mean and variance\n\nComputing moments\n\nThe random variable \\(X\\) has the following distribution:\n\n\n\n\\(X\\)\n0\n1\n2\n3\n\n\n\n\n\\(P(X)\\)\n0.4\n0.3\n0.2\n0.1\n\n\n\n\nCompute \\(E[X]\\).\n\nCompute \\(\\text{Var}(X)\\).\n\nCompute the standard deviation of \\(X\\).\n\nInterpret the expected value in words.\n\n\nLinear transformations\n\nLet \\(Y = 3X + 2\\).\n\nCompute \\(E[Y]\\).\n\nCompute \\(\\text{Var}(Y)\\).\n\nFind the standard deviation of \\(Y\\).\n\nExplain why adding a constant affects the mean but not the variance.\n\n\n\n\n7.9.3 Joint distributions\n\nMarginal distributions\n\nThe joint probability distribution of \\(X\\) and \\(Y\\) is:\n\n\n\n\n\\(Y=-1\\)\n\\(Y=0\\)\n\\(Y=1\\)\n\n\n\n\n\\(X=0\\)\n0.1\n0.1\n0.1\n\n\n\\(X=2\\)\n0.1\n0.2\n0.1\n\n\n\\(X=4\\)\n0.1\n0.1\n0.1\n\n\n\n\nCompute the marginal distribution of \\(X\\).\n\nCompute the marginal distribution of \\(Y\\).\n\nCompute \\(E[X]\\) and \\(E[Y]\\).\n\n\nIndependence and correlation\n\nUsing the table in Question 5:\n\nAre \\(X\\) and \\(Y\\) independent? Justify your answer.\n\nCompute \\(\\text{Cov}(X,Y)\\).\n\nCompute \\(\\text{Corr}(X,Y)\\).\n\nExplain why zero correlation does not necessarily imply independence.\n\n\n\n\n7.9.4 Counting and binomial reasoning\n\nThree coin tosses\n\nLet \\(X\\) be the number of heads when a fair coin is tossed three times.\n\nList all possible outcomes.\n\nConstruct the probability distribution of \\(X\\).\n\nCompute \\(E[X]\\) and \\(\\text{Var}(X)\\).\n\n\nFamilies and children\n\nAssume that the probability of a boy equals the probability of a girl and that births are independent.\n\nIn families with five children, what proportion have more girls than boys?\n\nList all relevant outcomes explicitly.\n\n\n\n\n7.9.5 Conceptual questions\n\nBig picture\n\n\nWhy is the expected value often called the “long-run average”?\n\nWhy is variance measured in squared units?\n\nGive one economic example where a discrete random variable is more appropriate than a continuous one.\n\nExplain in your own words the difference between independence and zero covariance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch8.html#footnotes",
    "href": "Ch8.html#footnotes",
    "title": "7  Discrete Probability Distribution",
    "section": "",
    "text": "The expected value is sometimes referred to as the population average; We will see later another definition of the expected value in later chpaters, as the “average” over infinite samples.↩︎\nIt is instructive to compare the expected value and variance rules with the summation rules.↩︎\nThese are sometimes referred to as moments–the mean and variance are the first moments and second central moments, respectively.\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{Order} & \\text{Raw Moment} & \\text{Central Moment} & \\text{Common Name} \\\\\n\\hline\n\\text{1st} & \\mu_1' = E[X] & \\mu_1 = E[X - E[X]] = 0 & \\text{Mean} \\\\\n\\hline\n\\text{2nd} & \\mu_2' = E[X^2] & \\mu_2 = E[(X - \\mu)^2] & \\text{Variance} \\\\\n\\hline\n\\text{3rd} & \\mu_3' & \\mu_3 = E[(X - \\mu)^3] & \\text{Skewness} \\\\\n\\hline\n\\text{4th} & \\mu_4' & \\mu_4 = E[(X - \\mu)^4] & \\text{Kurtosis} \\\\\n\\hline\n\\end{array}\n\\]↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Ch9.html",
    "href": "Ch9.html",
    "title": "8  Binomial and Poisson Distributions",
    "section": "",
    "text": "8.1 Counting principles\nBefore introducing the binomial formula, we need a short detour into counting.\nProbability often reduces to a simple question:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#counting-principles",
    "href": "Ch9.html#counting-principles",
    "title": "8  Binomial and Poisson Distributions",
    "section": "",
    "text": "How many possible outcomes are there?\n\n\n8.1.1 The multiplication rule of counting\nIf one task can be performed in \\(m\\) ways and a second task in \\(n\\) ways, then the two tasks together can be performed in \\(m \\times n\\) ways.\nExample:\nA restaurant offers 3 starters and 4 main courses.\nThe number of possible meals is:\n\\[\n3 \\times 4 = 12.\n\\]\nThis simple idea lies behind much of probability theory.\n\n\n\n8.1.2 Permutations\nA permutation counts the number of ways to arrange objects when order matters.\nThe number of ways to arrange \\(n\\) distinct objects is:\n\\[\nn! = n(n-1)(n-2)\\dots 1.\n\\]\nIf we choose \\(k\\) objects out of \\(n\\) and order matters, the number of permutations is:\n\\[\nP(n,k) = \\frac{n!}{(n-k)!}.\n\\]\n\n\n\n8.1.3 Combinations\nA combination counts the number of ways to choose objects when order does not matter.\nThe number of combinations of \\(k\\) objects from \\(n\\) is:\n\\[\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}.\n\\]\nThis expression is called the binomial coefficient.\nIt plays a central role in the binomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#the-binomial-distribution",
    "href": "Ch9.html#the-binomial-distribution",
    "title": "8  Binomial and Poisson Distributions",
    "section": "8.2 The Binomial Distribution",
    "text": "8.2 The Binomial Distribution\nWe often encounter problems of the form:\n\nA coin is tossed 4 times. What is the probability of exactly 1 head?\nA die is rolled 10 times. What is the probability of exactly 3 sixes?\nA box contains 1 red and 9 green marbles. Five draws are made with replacement. What is the probability of exactly 2 red marbles?\n\nThese are examples of binomial experiments.\n\n8.2.1 Conditions for a binomial model\nA binomial model applies when:\n\nThe number of trials \\(n\\) is fixed.\nEach trial has two possible outcomes (success/failure).\nThe probability of success \\(p\\) is constant.\nThe trials are independent.\n\n\n\n\n8.2.2 Box Model Example\nLet’s look more closely at the last example above. We might start with a box model:\n\n\n\n\n\n\nFigure 8.1: Box Model\n\n\n\nFrom the above, we draw 5 tickets at random with replacement. But we want exactly 2 R’s and 3 G’s (e.g., \\(\\fbox{R}, \\fbox{R}, \\fbox{G}, \\fbox{G}, \\fbox{G}\\) or \\(\\fbox{G}, \\fbox{R}, \\fbox{G}, \\fbox{G}, \\fbox{R}\\), etc.). The exact probability can be found using 3 simple steps:\n1) Find all the possible ways: \\[\n\\frac{5!}{2! \\times 3!} = \\frac{\\text{total cases}}{\\text{no. of successes} \\times \\text{no. of failures}}\n\\]\n2) Calculate the chance of each: \\[\n\\frac{1}{10} \\times \\frac{1}{10} \\times \\frac{9}{10} \\times \\frac{9}{10} \\times \\frac{9}{10} = \\left( \\frac{1}{10}\\right)^2 \\left( \\frac{9}{10}\\right)^3\n\\]\n3) Use the addition rule to add up the chances: \\[\n\\frac{5!}{2! \\times 3!} \\times \\left( \\frac{1}{10}\\right)^2 \\left( \\frac{9}{10}\\right)^3 \\approx 7\\%\n\\]\nThis can be interpreted as the chance of getting exactly 2 \\(\\fbox{R}\\)’s.\nNote that step 1 is actually the binomial coefficient, which gives us the total number of combinations of arranging 2 R’s and 3 G’s. In step 2, the chance of getting \\(\\fbox{R}, \\fbox{R}, \\fbox{G}, \\fbox{G}, \\fbox{G}\\) is the same as getting \\(\\fbox{G}, \\fbox{R}, \\fbox{G}, \\fbox{G}, \\fbox{R}\\), and so on. Lastly, step 3 involves the addition rules as all events are mutually exclusive (disjoint).\n\n\n8.2.3 The Binomial Formula\n\n\n\n\n\n\nImportantThe Binomial Probability Formula\n\n\n\nIf \\(X\\) counts the number of successes in \\(n\\) independent trials with success probability \\(p\\), then\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k},\n\\quad k = 0,1,\\dots,n.\n\\]\n\n\nWhere:\n\n\\(n\\) = number of trials\n\n\\(k\\) = number of successes\n\n\\(p\\) = probability of success\n\n\n\n\n8.2.4 Example\nA neighbour has 3 children. What is the probability exactly 2 are boys?\nAssuming \\(p = 1/2\\),\n\\[\nP(X=2) = \\binom{3}{2}\n\\left(\\frac{1}{2}\\right)^2\n\\left(\\frac{1}{2}\\right)^1\n= \\frac{3}{8}.\n\\]\nThis is exactly the same as the probability of 2 heads in 3 coin tosses.\n\n\n\n8.2.5 Mean and variance of the binomial\nIf \\(X \\sim \\text{Bin}(n,p)\\), then\n\\[\nE(X) = np\n\\]\n\\[\n\\text{Var}(X) = np(1-p)\n\\]\nThese formulas are extremely useful.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#normal-approximation-to-the-binomial-distribution",
    "href": "Ch9.html#normal-approximation-to-the-binomial-distribution",
    "title": "8  Binomial and Poisson Distributions",
    "section": "8.3 Normal approximation to the binomial distribution",
    "text": "8.3 Normal approximation to the binomial distribution\nWhen \\(n\\) is large, calculating binomial probabilities directly can be tedious.\nFortunately, the Central Limit Theorem tells us that the binomial distribution becomes approximately normal when \\(n\\) is large.\nIf\n\\[\nX \\sim \\text{Bin}(n,p),\n\\]\nthen for sufficiently large \\(n\\),\n\\[\nX \\approx N\\big(np,\\; np(1-p)\\big).\n\\]\nThat is:\n\nMean \\(= np\\)\nVariance \\(= np(1-p)\\)\nStandard deviation \\(= \\sqrt{np(1-p)}\\)\n\n\n\n8.3.1 When is the approximation good?\nA common rule of thumb is:\n\\[\nnp \\ge 5\n\\quad \\text{and} \\quad\nn(1-p) \\ge 5.\n\\]\n\n\n\n8.3.2 Continuity correction\nBecause the binomial distribution is discrete and the normal distribution is continuous, we use a continuity correction.\nFor example:\nTo approximate \\(P(X \\le 10)\\), compute\n\\[\nP(X \\le 10.5)\n\\]\nusing the normal distribution.\n\n\n\n8.3.3 Example\nSuppose \\(X \\sim \\text{Bin}(100, 0.5)\\).\nMean: \\[\n\\mu = np = 50\n\\]\nStandard deviation: \\[\n\\sigma = \\sqrt{100(0.5)(0.5)} = 5.\n\\]\nTo approximate\n\\[\nP(X \\ge 60),\n\\]\nwe compute\n\\[\nP\\left(Z \\ge \\frac{59.5 - 50}{5}\\right).\n\\]\nThe rest follows using the standard normal table.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#another-example-normal-approximation-to-the-binomial",
    "href": "Ch9.html#another-example-normal-approximation-to-the-binomial",
    "title": "8  Binomial and Poisson Distributions",
    "section": "8.4 Another Example: Normal Approximation to the Binomial",
    "text": "8.4 Another Example: Normal Approximation to the Binomial\nSuppose a fair coin is tossed \\(100\\) times.\nLet \\(X\\) be the number of heads.\nThen\n\\[\nX \\sim \\text{Bin}(100, 0.5).\n\\]\nStep 1: Compute mean and standard deviation\n\\[\n\\mu = np = 100(0.5) = 50\n\\]\n\\[\n\\sigma = \\sqrt{np(1-p)} = \\sqrt{100(0.5)(0.5)} = 5.\n\\]\n\nExample 1: Approximate \\(P(X \\ge 60)\\)\nStep 2: Apply continuity correction\nBecause the binomial is discrete, we approximate:\n\\[\nP(X \\ge 60) \\approx P(X \\ge 59.5).\n\\]\nStep 3: Convert to a \\(z\\)-score\n\\[\nz = \\frac{59.5 - 50}{5}\n= \\frac{9.5}{5}\n= 1.9.\n\\]\nStep 4: Use the standard normal table\nFrom the table:\n\\[\nP(Z \\le 1.9) = 0.9713.\n\\]\nTherefore,\n\\[\nP(Z \\ge 1.9) = 1 - 0.9713 = 0.0287.\n\\]\nFinal Answer\n\\[\nP(X \\ge 60) \\approx 0.0287.\n\\]\nSo there is roughly a \\(2.9\\%\\) chance of obtaining 60 or more heads.\n\nExample 2: Approximate \\(P(45 \\le X \\le 55)\\)\nStep 2: Continuity correction\n\\[\nP(45 \\le X \\le 55)\n\\approx\nP(44.5 \\le X \\le 55.5).\n\\]\nStep 3: Convert to \\(z\\)-scores\nLower bound:\n\\[\nz_1 = \\frac{44.5 - 50}{5}\n= -1.1.\n\\]\nUpper bound:\n\\[\nz_2 = \\frac{55.5 - 50}{5}\n= 1.1.\n\\]\nStep 3: Look up table values\nFrom the standard normal table:\n\\[\nP(0 \\le Z \\le 1.1) = 0.3643.\n\\]\nBy symmetry:\n\\[\nP(-1.1 \\le Z \\le 1.1)\n= 2(0.3643)\n= 0.7286.\n\\]\n*Final Answer**\n\\[\nP(45 \\le X \\le 55) \\approx 0.7286.\n\\]\nSo about \\(73\\%\\) of the time, the number of heads will fall between 45 and 55.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#the-poisson-distribution",
    "href": "Ch9.html#the-poisson-distribution",
    "title": "8  Binomial and Poisson Distributions",
    "section": "8.5 The Poisson Distribution",
    "text": "8.5 The Poisson Distribution\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space when events occur:\n\nindependently,\nrandomly,\nat a constant average rate.\n\nTypical examples:\n\nNumber of calls per hour,\nNumber of defects per page,\nNumber of accidents per month.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#the-poisson-formula",
    "href": "Ch9.html#the-poisson-formula",
    "title": "8  Binomial and Poisson Distributions",
    "section": "8.6 The Poisson Formula",
    "text": "8.6 The Poisson Formula\n\n\n\n\n\n\nImportantThe Poisson Probability Function\n\n\n\nIf \\(X\\) follows a Poisson distribution with mean \\(\\mu\\), then\n\\[\nP(X = x) = \\frac{e^{-\\mu} \\mu^x}{x!},\n\\quad x = 0,1,2,\\dots\n\\]\n\n\nWhere:\n\n\\(\\mu\\) = average number of events,\n\\(e \\approx 2.71828\\).\n\n\n\n8.6.1 Mean and variance\nFor a Poisson distribution:\n\\[\nE(X) = \\mu\n\\]\n\\[\n\\text{Var}(X) = \\mu.\n\\]\nThus, the mean and variance are equal.\n\n\n\n8.6.2 Example\nIf typographical errors follow a Poisson distribution with mean \\(\\mu = 1.5\\) per 100 pages, then\n\\[\nP(X=0) = \\frac{e^{-1.5} 1.5^0}{0!}\n= e^{-1.5}\n\\approx 0.2231.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#binomialpoisson-connection",
    "href": "Ch9.html#binomialpoisson-connection",
    "title": "8  Binomial and Poisson Distributions",
    "section": "8.7 Binomial–Poisson connection",
    "text": "8.7 Binomial–Poisson connection\nWhen:\n\n\\(n\\) is large,\n\\(p\\) is small,\nand \\(\\mu = np\\) remains moderate,\n\nthe binomial distribution can be approximated by a Poisson distribution:\n\\[\n\\text{Bin}(n,p) \\approx \\text{Poisson}(\\mu = np).\n\\]\nThis is especially useful for modeling rare events.\n\n\n\n\n\n\nImportantChapter Summary\n\n\n\nThe binomial distribution models the number of successes in repeated independent trials with constant probability \\(p\\), while the Poisson distribution models rare events occurring over time or space with average rate \\(\\mu\\). The binomial has mean \\(np\\) and variance \\(np(1-p)\\); the Poisson has mean and variance both equal to \\(\\mu\\). For large samples, the binomial distribution can be approximated by the normal distribution using standardization with \\(z\\)-scores. Together, these models allow us to quantify uncertainty in repeated trials, rare events, and large-sample settings.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch9.html#exercises",
    "href": "Ch9.html#exercises",
    "title": "8  Binomial and Poisson Distributions",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\n\n8.8.1 Conceptual Understanding\n\nWhat are the four conditions required for a binomial experiment?\nExplain why independence of trials is crucial for the binomial formula.\nWhen is the Poisson distribution more appropriate than the binomial?\nWhy does the normal approximation improve as \\(n\\) increases?\n\n\n\n\n8.8.2 Binomial Calculations\n\nA multiple-choice test has 20 questions with 4 possible answers each.\n\nIf a student guesses randomly, what is the probability of exactly 8 correct answers?\n\nWhat are the mean and standard deviation?\n\nA manufacturing process produces defective items with probability \\(p = 0.02\\).\n\nIn a batch of 50 items, what is the probability of exactly 1 defect?\n\nWhat is the probability of no defects?\n\n\n\n\n\n8.8.3 Normal Approximation\n\nSuppose \\(X \\sim \\text{Bin}(200, 0.4)\\).\n\nCompute the mean and standard deviation.\n\nApproximate \\(P(X \\ge 90)\\) using the normal approximation.\n\nClearly show the continuity correction and \\(z\\)-steps.\n\nA factory produces lightbulbs with defect probability \\(p = 0.1\\).\nIn a shipment of 120 bulbs:\n\nApproximate the probability that between 8 and 18 bulbs are defective.\n\nCompare your answer to what you expect intuitively.\n\n\n\n\n\n8.8.4 Poisson Applications\n\nCustomers arrive at a bank at an average rate of 3 per minute.\n\nWhat is the probability that exactly 5 customers arrive in one minute?\n\nWhat is the probability that no customers arrive in two minutes?\n\nCalls arrive at a call center at a rate of 2 per hour.\n\nWhat is the probability of 6 or more calls in one hour?\n\nWhat is the probability of exactly 1 call in 30 minutes?\n\n\n\n\n\n8.8.5 Synthesis\n\nUnder what conditions can:\n\n\nThe binomial be approximated by the normal?\n\nThe binomial be approximated by the Poisson?\n\n\nSuppose \\(n\\) is very large and \\(p\\) is very small.\n\nExplain intuitively why the Poisson distribution emerges from the binomial.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binomial and Poisson Distributions</span>"
    ]
  },
  {
    "objectID": "Ch10.html",
    "href": "Ch10.html",
    "title": "9  Uniform and Exponential Probability Distributions",
    "section": "",
    "text": "9.1 The Uniform Distribution\nIn contrast to a discrete random variable, a continuous random variable can assume an uncountable number of values.\nBecause there are infinitely many possible values, the probability of any single point is zero:\n\\[\nP(X = x) = 0.\n\\]\nInstead, probabilities are defined over intervals, such as\n\\[\nP(x_1 \\le X \\le x_2), \\quad x_1 &lt; x_2.\n\\]\nFor this reason, continuous random variables are described by a probability density function (p.d.f.), denoted \\(f(x)\\).\nFor a function \\(f(x)\\) defined on \\(a \\le x \\le b\\) to be a valid p.d.f., two conditions must hold:\n\\[\n\\int_a^b f(x)\\,dx = 1.\n\\]\nThe uniform distribution (also called the rectangular distribution) is the simplest continuous model.\nIts probability density function is:\n\\[\nf(x) = \\frac{1}{b-a}, \\quad a \\le x \\le b,\n\\]\nand \\(f(x) = 0\\) outside this interval.\nBecause the density is constant, probabilities are simply areas of rectangles.\nTo compute\n\\[\nP(x_1 \\le X \\le x_2),\n\\]\nwe calculate:\n\\[\nP(x_1 \\le X \\le x_2)\n= (x_2 - x_1)\\cdot \\frac{1}{b-a}.\n\\]\nThe mean and variance are:\n\\[\nE(X) = \\frac{a+b}{2},\n\\]\n\\[\nVar(X) = \\frac{(b-a)^2}{12}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Uniform and Exponential Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch10.html#the-uniform-distribution",
    "href": "Ch10.html#the-uniform-distribution",
    "title": "9  Uniform and Exponential Probability Distributions",
    "section": "",
    "text": "ImportantKey intuition\n\n\n\nUnder a uniform distribution, all values in the interval are equally likely.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Uniform and Exponential Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch10.html#the-exponential-distribution",
    "href": "Ch10.html#the-exponential-distribution",
    "title": "9  Uniform and Exponential Probability Distributions",
    "section": "9.2 The Exponential Distribution",
    "text": "9.2 The Exponential Distribution\nAnother important continuous model is the exponential distribution, often used for waiting-time problems.\nIts probability density function is:\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0,\n\\]\nwhere:\n\n\\(\\lambda &gt; 0\\) is the rate parameter,\n\n\\(e \\approx 2.71828\\).\n\n\n\n\n\n\nThe exponential distribution is commonly used to model:\n\ntime until a job is found,\ntime until equipment failure,\nwaiting time between arrivals,\nlifetime of components,\nother “time-to-event” phenomena.\n\n\n\n9.2.1 Mean and standard deviation\nA particularly convenient property is:\n\\[\nE(X) = \\frac{1}{\\lambda}, \\qquad\nSD(X) = \\frac{1}{\\lambda}.\n\\]\nThus, for the exponential distribution, the mean and standard deviation are equal.\nSmaller values of \\(\\lambda\\) imply a flatter curve and a larger expected waiting time.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Uniform and Exponential Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch10.html#cumulative-probabilities",
    "href": "Ch10.html#cumulative-probabilities",
    "title": "9  Uniform and Exponential Probability Distributions",
    "section": "9.3 Cumulative probabilities",
    "text": "9.3 Cumulative probabilities\nFrom calculus, we obtain the cumulative distribution function:\n\\[\nP(X &gt; x) = e^{-\\lambda x}.\n\\]\nTherefore,\n\\[\nP(X &lt; x) = 1 - e^{-\\lambda x}.\n\\]\nMore generally,\n\\[\nP(x_1 &lt; X &lt; x_2)\n= P(X &lt; x_2) - P(X &lt; x_1)\n= e^{-\\lambda x_1} - e^{-\\lambda x_2}.\n\\]\n\n\n\n\n\n\nNoteMemoryless property\n\n\n\nThe exponential distribution has a special feature called the memoryless property:\n\\[\nP(X &gt; s+t \\mid X &gt; s) = P(X &gt; t).\n\\]\nThe remaining waiting time does not depend on how long you have already waited.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Uniform and Exponential Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch10.html#worked-example-exponential-waiting-time",
    "href": "Ch10.html#worked-example-exponential-waiting-time",
    "title": "9  Uniform and Exponential Probability Distributions",
    "section": "9.4 Worked Example: Exponential Waiting Time",
    "text": "9.4 Worked Example: Exponential Waiting Time\nSuppose the waiting time (in hours) until a machine fails follows an exponential distribution with rate\n\\[\n\\lambda = 0.5.\n\\]\nThis means the average failure rate is 0.5 per hour.\nStep 1: Find the mean waiting time\nFor an exponential distribution,\n\\[\nE(X) = \\frac{1}{\\lambda}.\n\\]\nTherefore,\n\\[\nE(X) = \\frac{1}{0.5} = 2 \\text{ hours}.\n\\]\nSo on average, the machine lasts 2 hours before failing.\nStep 2: Probability the machine lasts more than 3 hours\nWe use the survival function:\n\\[\nP(X &gt; x) = e^{-\\lambda x}.\n\\]\nThus,\n\\[\nP(X &gt; 3) = e^{-0.5 \\cdot 3}\n= e^{-1.5}.\n\\]\nUsing a calculator,\n\\[\ne^{-1.5} \\approx 0.2231.\n\\]\nSo there is about a 22.3% chance the machine lasts longer than 3 hours.\nStep 3: Probability the machine fails within 1 hour\nWe use\n\\[\nP(X &lt; x) = 1 - e^{-\\lambda x}.\n\\]\nHence,\n\\[\nP(X &lt; 1) = 1 - e^{-0.5 \\cdot 1}\n= 1 - e^{-0.5}.\n\\]\nSince\n\\[\ne^{-0.5} \\approx 0.6065,\n\\]\nwe get\n\\[\nP(X &lt; 1) = 1 - 0.6065 = 0.3935.\n\\]\nSo there is about a 39.4% chance the machine fails within 1 hour.\nStep 4: Demonstrating the memoryless property\nSuppose the machine has already survived 2 hours.\nWhat is the probability it survives 2 more hours?\nUsing the memoryless property:\n\\[\nP(X &gt; 4 \\mid X &gt; 2) = P(X &gt; 2).\n\\]\nNow compute:\n\\[\nP(X &gt; 2) = e^{-0.5 \\cdot 2}\n= e^{-1}\n\\approx 0.3679.\n\\]\nNotice something remarkable:\nThe probability of surviving 2 more hours does not depend on the fact that it already survived 2 hours.\nThat is the memoryless property in action.\n\n\n\n\n\n\nImportantChapter summary\n\n\n\nContinuous random variables assign probability to intervals rather than individual points.\nIn this chapter, we introduced:\n\nthe defining properties of probability density functions,\nthe uniform distribution, where all values in an interval are equally likely,\nthe exponential distribution, a fundamental model for waiting times.\n\nThese distributions play important roles in economics, reliability analysis, and queueing theory.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Uniform and Exponential Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch10.html#exercises",
    "href": "Ch10.html#exercises",
    "title": "9  Uniform and Exponential Probability Distributions",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n\n9.5.1 Job Search Duration (Exponential Model)\nSuppose the time (in months) it takes a job seeker to find employment follows an exponential distribution with rate \\(\\lambda = 0.25\\).\n\nWhat is the expected duration of unemployment?\nWhat is the probability the individual remains unemployed for more than 6 months?\nWhat is the probability the individual finds a job within 3 months?\nSuppose the individual has already been unemployed for 4 months. What is the probability they remain unemployed for at least 2 more months? Explain your answer economically.\n\n\n\n\n9.5.2 Machine Breakdowns (Exponential Model)\nA factory machine breaks down according to an exponential distribution with mean lifetime of 5 years.\n\nWhat is the value of \\(\\lambda\\)?\nWhat is the probability the machine lasts more than 8 years?\nWhat is the probability the machine fails between years 2 and 4?\nIf the machine has already lasted 5 years, does its expected remaining lifetime increase, decrease, or stay the same? Briefly explain.\n\n\n\n\n9.5.3 Uniform Distribution: Consumer Willingness to Pay\nSuppose consumers’ willingness to pay (WTP) for a product is uniformly distributed between $10 and $50.\n\nWhat is the mean willingness to pay?\nIf the firm sets a price of $30, what fraction of consumers will purchase the product?\nWhat price would ensure exactly 25% of consumers buy?\nInterpret economically what it means that the distribution is uniform.\n\n\n\n\n9.5.4 Arrival of Customers (Exponential Waiting Time)\nCustomers arrive at a café according to an exponential waiting time distribution with mean waiting time of 10 minutes.\n\nWhat is the arrival rate per minute?\nWhat is the probability that the next customer arrives within 5 minutes?\nWhat is the probability that the café experiences a waiting time longer than 15 minutes?\nWhy is the exponential distribution appropriate for modeling arrival times?\n\n\n\n\n9.5.5 Comparing Models (Conceptual)\nFor each situation below, indicate whether a uniform or exponential distribution would be more appropriate. Justify briefly.\n\nThe time until the next earthquake.\nThe resale price of used textbooks uniformly discounted between 40% and 60%.\nThe time until a randomly chosen unemployed worker finds a job.\nThe location of a buyer’s ideal price within a fixed bargaining range.\n\n\n\n\n\n\n\n\nImportantBig Picture Reflection\n\n\n\nIn applied economics, probability models are not chosen randomly.\n\nThe uniform distribution models situations with equal likelihood within a fixed range.\nThe exponential distribution models waiting times and rare, memoryless events.\n\nUnderstanding the economic mechanism helps determine the correct probabilistic model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Uniform and Exponential Probability Distributions</span>"
    ]
  }
]